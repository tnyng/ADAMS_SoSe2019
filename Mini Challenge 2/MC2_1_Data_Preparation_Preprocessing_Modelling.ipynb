{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MC2_1_Data_Preparation_Preprocessing_Modelling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y7hSVHg8nJ-d"
      },
      "source": [
        "# Mini-Challenge #2: predicting the book rating according to a review\n",
        "## Team: White Tea \n",
        "### Members: Yang Tian (Matr.-Nr.: 5074001, FU Berlin), Yu Fan (Matr.-Nr.: 5064892, FU Berlin)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0SCGmY03nMhL",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-YWhxGV-dtkh",
        "outputId": "2101e5a6-ccda-41c7-9fd1-4ed81359430c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# loading training and test data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "tr_PATH = './gdrive/My Drive/DL/Projects/Book Rating/MC2.csv' # training set\n",
        "ts_PATH = './gdrive/My Drive/DL/Projects/Book Rating/MC2test.csv' # test set\n",
        "\n",
        "tr = pd.read_csv(tr_PATH).set_index('Unnamed: 0')\n",
        "ts = pd.read_csv(ts_PATH).set_index('Unnamed: 0')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sMwgTuWFfs8D"
      },
      "source": [
        "### 1. Overview of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g-05-nJdd2jA",
        "outputId": "823838e4-38fe-4d6f-a8d7-150f3ff6554d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        }
      },
      "source": [
        "tr.info()\n",
        "tr.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1099209 entries, 1 to 1099209\n",
            "Data columns (total 8 columns):\n",
            "product_id           1099209 non-null object\n",
            "product_title        1099209 non-null object\n",
            "star_rating          1099209 non-null int64\n",
            "helpful_votes        1099209 non-null int64\n",
            "total_votes          1099209 non-null int64\n",
            "verified_purchase    1099209 non-null object\n",
            "review_headline      1099196 non-null object\n",
            "review_body          1099207 non-null object\n",
            "dtypes: int64(3), object(5)\n",
            "memory usage: 75.5+ MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>product_id</th>\n",
              "      <th>product_title</th>\n",
              "      <th>star_rating</th>\n",
              "      <th>helpful_votes</th>\n",
              "      <th>total_votes</th>\n",
              "      <th>verified_purchase</th>\n",
              "      <th>review_headline</th>\n",
              "      <th>review_body</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0983797706</td>\n",
              "      <td>Igniting Your True Purpose and Passion: A Busi...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>This is an inspirational and insightful book t...</td>\n",
              "      <td>This is an inspirational and insightful book t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1423151283</td>\n",
              "      <td>The Duckling Gets a Cookie!? (Pigeon)</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Loved it!</td>\n",
              "      <td>My twins are 3 and they love the pigeon books!...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1445604752</td>\n",
              "      <td>Spitfire Ace of Aces: The Wartime Story of Joh...</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>N</td>\n",
              "      <td>Engaging Account of the Combat Career of an RA...</td>\n",
              "      <td>Back in 1964, I was introduced to 'Johnnie' Jo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0393057941</td>\n",
              "      <td>The Bread Bible</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Great book for beginners</td>\n",
              "      <td>I disagree with those reviews that say this is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1608322858</td>\n",
              "      <td>Do It Well. Make It Fun.: The Key to Success i...</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>N</td>\n",
              "      <td>Left me wanting more</td>\n",
              "      <td>Reading this book, I hoped to find more about ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>B001FVQAPW</td>\n",
              "      <td>Myth and Christianity: An Inquiry into the Pos...</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>N</td>\n",
              "      <td>A \"SEMI-DIALOGUE\" BETWEEN TWO TOWERING 20TH CE...</td>\n",
              "      <td>Rudolf Bultmann (1884-1976) was a German theol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0307157857</td>\n",
              "      <td>Richard Scarry's Cars and Trucks and Things Th...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Best Toddler Boy Picture Book</td>\n",
              "      <td>My son has adored this book since he could sit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0670022969</td>\n",
              "      <td>American Nations: A History of the Eleven Riva...</td>\n",
              "      <td>5</td>\n",
              "      <td>370</td>\n",
              "      <td>388</td>\n",
              "      <td>Y</td>\n",
              "      <td>Like scales falling from my eyes</td>\n",
              "      <td>Colin Woodard has written the story of North A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>193260393X</td>\n",
              "      <td>A Caregiver's Guide to Lewy Body Dementia</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>N</td>\n",
              "      <td>Disappointed and not worth the money</td>\n",
              "      <td>I was overall, very disappointed with this boo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1461164591</td>\n",
              "      <td>Castles: A Fictional Memoir of a Girl with Sci...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Y</td>\n",
              "      <td>Wretlind makes reading this difficult story a joy</td>\n",
              "      <td>Often, you can tell on the first page whether ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            product_id  ...                                        review_body\n",
              "Unnamed: 0              ...                                                   \n",
              "1           0983797706  ...  This is an inspirational and insightful book t...\n",
              "2           1423151283  ...  My twins are 3 and they love the pigeon books!...\n",
              "3           1445604752  ...  Back in 1964, I was introduced to 'Johnnie' Jo...\n",
              "4           0393057941  ...  I disagree with those reviews that say this is...\n",
              "5           1608322858  ...  Reading this book, I hoped to find more about ...\n",
              "6           B001FVQAPW  ...  Rudolf Bultmann (1884-1976) was a German theol...\n",
              "7           0307157857  ...  My son has adored this book since he could sit...\n",
              "8           0670022969  ...  Colin Woodard has written the story of North A...\n",
              "9           193260393X  ...  I was overall, very disappointed with this boo...\n",
              "10          1461164591  ...  Often, you can tell on the first page whether ...\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b0sePARsXhpD",
        "outputId": "fd9d480c-3a35-4703-e4b4-074f016c7492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# Checking the Na/NaN values in the data frame\n",
        "\n",
        "def findNaN(df, sum=True):\n",
        "  if sum==True:\n",
        "    return np.sum(df.isnull())\n",
        "  else:\n",
        "    return np.where(df.isnull())\n",
        "  \n",
        "print(tr.isnull().sum(axis=0), '\\n')\n",
        "\n",
        "print(\"Total number of records: \", tr.shape[0])\n",
        "print(\"Number of records with unique product ID: \", np.unique(tr[\"product_id\"]).shape[0], \"\\n\")\n",
        "print(\"Number of Missings in the Dataset\\n\")\n",
        "print(\"Number of missings in product ID: \", findNaN(tr.product_id))\n",
        "print(\"Number of missings in Headline: \", findNaN(tr.review_headline))\n",
        "print(\"Number of missings in Body: \", findNaN(tr.review_body), \"\\n\")\n",
        "print(\"Index of missings in the Dataset\\n\")\n",
        "print(\"Index of missings in the review headline: \\n\", findNaN(tr.review_headline, False))\n",
        "print(\"Index of missings in the review body: \\n\", findNaN(tr.review_body, False))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "product_id            0\n",
            "product_title         0\n",
            "star_rating           0\n",
            "helpful_votes         0\n",
            "total_votes           0\n",
            "verified_purchase     0\n",
            "review_headline      13\n",
            "review_body           2\n",
            "dtype: int64 \n",
            "\n",
            "Total number of records:  1099209\n",
            "Number of records with unique product ID:  540434 \n",
            "\n",
            "Number of Missings in the Dataset\n",
            "\n",
            "Number of missings in product ID:  0\n",
            "Number of missings in Headline:  13\n",
            "Number of missings in Body:  2 \n",
            "\n",
            "Index of missings in the Dataset\n",
            "\n",
            "Index of missings in the review headline: \n",
            " (array([  40304,  180433,  573567,  646587,  903807,  922540,  958151,\n",
            "        995901,  998520, 1005610, 1046110, 1049675, 1096752]),)\n",
            "Index of missings in the review body: \n",
            " (array([ 716204, 1077127]),)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NKE5NQ4XlmpL"
      },
      "source": [
        "#### Since there is no overlap between NaNs in the review body and those in the review headline, it will not hurt us if we combine these two parts together before conducting word embedding. Otherwise, rows with NaNs can be deleted. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PbQOijps3lK8"
      },
      "source": [
        "#### Let's have a look at our target variable - the star rating. In total, there are 5 levels of the ratings. In the following model we will create, multiclass classification should be the right strategy and 5 classes should be predicted. Specifically, the dataset is imbalanced, where the level of \"5 star rating\" occupies about 61%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "to7pLrr1g6bS",
        "outputId": "768d8a8c-dc37-4f6c-a590-587883021f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "# Checking the rating distribution\n",
        "\n",
        "v = pd.Series(tr['star_rating']).value_counts()\n",
        "print(v, \"\\n\")\n",
        "\n",
        "print(\"Percentage of five-star ratings: \", v.iloc[0] / v.sum())\n",
        "print(\"Percentage of positive ratings: \", v.iloc[:2].sum() / v.sum())\n",
        "print(\"Percentage of negative ratings: \", v.iloc[3:].sum() / v.sum(), \"\\n\")\n",
        "\n",
        "v.plot(kind='bar')\n",
        "plt.xlabel('# star ratings')\n",
        "plt.ylabel('count')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5    675734\n",
            "4    208329\n",
            "3     91574\n",
            "1     69240\n",
            "2     54332\n",
            "Name: star_rating, dtype: int64 \n",
            "\n",
            "Percentage of five-star ratings:  0.6147456944038849\n",
            "Percentage of positive ratings:  0.8042719810336342\n",
            "Percentage of negative ratings:  0.11241902131441792 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'count')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAEJCAYAAACkH0H0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVfX/AWkQy0nrC4jzAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Idbe30t0t7rK",
        "outputId": "e4035ad9-453e-4d9a-93d3-3e2d0bb4e945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "ID = pd.Series(tr['product_id']).value_counts()\n",
        "ID.iloc[:30].plot(kind='bar')\n",
        "plt.xlabel('# product IDs')\n",
        "plt.ylabel('count')\n",
        "\n",
        "print(ID.iloc[:30], '\\n')\n",
        "print(\"The volume of top-100 best sellers ranges from {} to {}.\".format(ID.iloc[0], ID.iloc[100]), '\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "031606792X    822\n",
            "0964729237    648\n",
            "0316015849    581\n",
            "0545010225    547\n",
            "0399155341    473\n",
            "0805093079    433\n",
            "0316031844    426\n",
            "0143038257    417\n",
            "1582701709    399\n",
            "0439023513    383\n",
            "0143038419    380\n",
            "0385504225    375\n",
            "1416562850    372\n",
            "1565125606    357\n",
            "0307454541    341\n",
            "0439023521    340\n",
            "0849946158    327\n",
            "1594480001    290\n",
            "0316160202    283\n",
            "1892112000    279\n",
            "030746363X    268\n",
            "1400064163    259\n",
            "0553801473    244\n",
            "0762424931    237\n",
            "074324754X    237\n",
            "0439023491    237\n",
            "1439173249    228\n",
            "0439023483    226\n",
            "0061939897    224\n",
            "015602943X    221\n",
            "Name: product_id, dtype: int64 \n",
            "\n",
            "The volume of top-100 best sellers ranges from 822 to 111. \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAFDCAYAAADLWyJSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXm8bvX0x9+nbqU5Orq61U+lkjEU\nRaKRIpXwUYYG5fqRQqJSJBmKBvFDbuPNUC3RgCaaCGkiIkOldG+TS4Mkqnt+f6zvvmc/+9nP2ft7\npvvc03q/Xud1nmfvtb/P2tN3fYf1XWtgaGiIIAiCIBiJxRa2AkEQBEH/E8YiCIIgaCSMRRAEQdBI\nGIsgCIKgkTAWQRAEQSNhLIIgCIJGwlgEQRAEjYSxCIIgCBoJYxEEQRA0Mm1hKzCOxFL0IAiC0THQ\nJDCVjAV33XVX17bBwUHmzZvX6vipLNsvevSDbL/o0Q+y/aLHoibbL3qMh+yMGTNaHR/DUEEQBEEj\nYSyCIAiCRsJYBEEQBI2EsQiCIAgaCWMRBEEQNBLGIgiCIGgkjEUQBEHQSBiLIAiCoJEwFkEQBEEj\nU2oFd8ET795hwed70//FTzx/4SgTBEEwBYieRRAEQdDIpPUsJH0I2BsP+PdbYE9gVeBMYGXgeuCd\nZvZfSUsBpwMbAn8H3mpmt0+WrkEQBEEnk9KzkLQasB+wkZk9H1gc2AU4CjjOzNYB7gf2SofsBdyf\nth+X5IIgCIKFxGQOQ00DlpY0DVgGuBvYEjg77Z8N7JQ+75i+k/ZvJakxhG4QBEEwMUzKMJSZzZV0\nNPBX4N/AJfiw0wNm9ngSmwOslj6vBtyZjn1c0oP4UFVHfF1JM4GZSY7BwUFgeFK7TLFvJKZNm9ZK\nblGU7Rc9+kG2X/ToB9l+0WNRk+0XPSby/LqOH/WRGUh6Kt5bWAt4APgOsO1YyzWzWcCs9HVopLju\nbWK+93PM+bHK9ose/SDbL3r0g2y/6LGoyfaLHlMxn8XWwF/M7G9m9hjwPWBTYKU0LAWwOjA3fZ4L\nrAGQ9q+IT3QHQRAEC4HJ8ob6K7CJpGXwYaitgOuAy4E34x5RuwPnJfnz0/dfpP2XmVmkTQ2CIFhI\nTErPwsx+iU9U34C7zS6GDx8dCOwv6RZ8TuLkdMjJwMpp+/7AQZOhZxAEQVDPpK2zMLPDgMMqm28D\nXlYj+yjwlsnQKwiCIGgmVnAHQRAEjYSxCIIgCBoJYxEEQRA0EsYiCIIgaCSMRRAEQdBIGIsgCIKg\nkTAWQRAEQSNhLIIgCIJGwlgEQRAEjYSxCIIgCBoJYxEEQRA0EsYiCIIgaCSMRRAEQdBIGIsgCIKg\nkTAWQRAEQSNhLIIgCIJGJiX5kaRnA2eVNq0NfAI4PW1fE7gdkJndL2kAOB54HfAIsIeZ3TAZugZB\nEATdTFZa1T+a2YvM7EXAhrgBOAdPl3qpma0LXMpw+tTtgHXT30zga5OhZxAEQVDPwhiG2gq41czu\nAHYEZqfts4Gd0ucdgdPNbMjMrgZWkrTq5KsaBEEQwMIxFrsAZ6TP083s7vT5HmB6+rwacGfpmDlp\nWxAEQbAQmJQ5iwJJSwI7AAdX95nZkKShzPJm4sNUmBmDg4MA3FsjW+wbiWnTprWSWxRl+0WPfpDt\nFz36QbZf9FjUZPtFj4k8v67jR33k6NgOuMHMivr8XkmrmtndaZjpvrR9LrBG6bjV07YOzGwWMCt9\nHZo3b17PHx5pX8Hg4GAruUVRtl/06AfZftGjH2T7RY9FTbZf9BgP2RkzZrQ6frKNxa4MD0EBnA/s\nDhyZ/p9X2v5+SWcCGwMPloargiAIgklm0oyFpGWBbYD3lDYfCZikvYA7AKXtF+Bus7fgnlN7Tpae\nQRAEQTeTZizM7F/AypVtf8e9o6qyQ8A+k6RaEARB0ECs4A6CIAgaCWMRBEEQNBLGIgiCIGgkjEUQ\nBEHQSBiLIAiCoJEwFkEQBEEjYSyCIAiCRsJYBEEQBI1MdriPvuOJd++w4HM5AOHiJ54/+coEQRD0\nKdGzCIIgCBoJYxEEQRA0EsYiCIIgaCSMRRAEQdBIGIsgCIKgkTAWQRAEQSNhLIIgCIJGJjNT3krA\nScDzgSHgXcAfgbOANYHbAZnZ/ZIGgOPxbHmPAHuY2Q2TpWsQBEHQyWT2LI4HLjKz9YENgJuBg4BL\nzWxd4NL0HWA7YN30NxP42iTqGQRBEFSYFGMhaUXgVcDJAGb2XzN7ANgRmJ3EZgM7pc87Aqeb2ZCZ\nXQ2sJGnVydA1CIIg6GayhqHWAv4GnCppA+B64APAdDO7O8ncA0xPn1cD7iwdPydtu5sgCIJg0pks\nYzENeAmwr5n9UtLxDA85AWBmQ5KGcgqVNBMfpsLMGBwcBDpjPBUU+6rUyY4kXzBt2rRGmX6S7Rc9\n+kG2X/ToB9l+0WNRk+0XPSby/LqOH/WRecwB5pjZL9P3s3Fjca+kVc3s7jTMdF/aPxdYo3T86mlb\nB2Y2C5iVvg7NmzevpwIj7RuN/ODgYOsy+0G2X/ToB9l+0aMfZPtFj0VNtl/0GA/ZGTNmtDp+UuYs\nzOwe4E5Jz06btgJ+D5wP7J627Q6clz6fD+wmaUDSJsCDpeGqIAiCYJKZzBDl+wLfkrQkcBuwJ26s\nTNJewB2AkuwFuNvsLbjr7J6TqGcQBEFQYdKMhZn9GtioZtdWNbJDwD4TrlQQBEHQiid98qMcIlFS\nEARPViLcRxAEQdBIGIsgCIKgkTAWQRAEQSNhLIIgCIJGwlgEQRAEjYSxCIIgCBoJYxEEQRA0Euss\nJohYkxEEwVQiehZBEARBI2EsgiAIgkbCWARBEASNhLEIgiAIGgljEQRBEDQSxiIIgiBoJIxFEARB\n0MikrbOQdDvwT+AJ4HEz20jS04CzgDWB2wGZ2f2SBoDj8Wx5jwB7mNkNk6VrEARB0Mlk9yy2MLMX\nmVmRMe8g4FIzWxe4NH0H2A5YN/3NBL42yXoGQRAEJRb2MNSOwOz0eTawU2n76WY2ZGZXAytJWnVh\nKBgEQRBMrrEYAi6RdL2kmWnbdDO7O32+B5iePq8G3Fk6dk7aFgRBECwEJjM21CvNbK6kVYAfSfpD\neaeZDUkayikwGZ2Z6XgGBweBzlhMBcW+KnWyveQnSrbMtGnTGmVGIzuRZS9qsv2iRz/I9osei5ps\nv+gxkefXdfyoj8zEzOam//dJOgd4GXCvpFXN7O40zHRfEp8LrFE6fPW0rVrmLGBW+jo0b968nr8/\n0r6xyo+n7ODgYOvycmQnsuxFTbZf9OgH2X7RY1GT7Rc9xkN2xowZrY5vPQwl6YAe2/dvceyykpYv\nPgOvAW4Czgd2T2K7A+elz+cDu0kakLQJ8GBpuCoIgiCYZHLmLD7RY/uhLY6dDlwl6UbgGuCHZnYR\ncCSwjaQ/A1un7wAXALcBtwAnAu/L0DMIgiAYZxqHoSRtmT4uLmkLYKC0e2187cSImNltwAY12/8O\nbFWzfQjYp6ncIAiCYHJoM2dxcvr/FOCU0vYh3INp3/FWKgiCIOgvGo2Fma0FIOl0M9tt4lUKgiAI\n+o3W3lBlQyFpscq++eOp1JORIg1rpGANgqAfaW0sJL0E+ArwQnxICnz+YghYfPxVC4IgCPqFnHUW\ns4HvA+/Cg/sFQRAETxJyjMUzgUOSp1IQBEHwJCJnncU5+GK6IAiC4ElGTs/iKcA5kq7CXWYXEF5S\nQRAEU5scY/H79BcEQRA8ychxnT18IhUJgiAI+pcc19kte+0zs8vGR50gCIKgH8kZhjq58v3pwJJ4\nYqK1x02jIAiCoO/IGYZaq/xd0uJ4xNnGQILB+BKrvYMgmGxGnVbVzJ4APgN8dPzUCYIgCPqRsWbK\n2waIuFB9TNELgeGeSPRCgiDIJWeC+048DlTBMvjai0hMFARBMMXJ6Vm8o/L9X8CfzOyhtgWkeY7r\ngLlmtr2ktYAzgZWB64F3mtl/JS0FnA5sCPwdeKuZ3Z6hazAK6nohED2RIAgy5izM7EozuxL4KfAn\n4IYcQ5H4AHBz6ftRwHFmtg5wP7BX2r4XcH/aflySC4IgCBYSrY2FpOUlnQ78G5gL/FvSbEkrtjx+\ndeD1wEnp+wCwJXB2EpkN7JQ+75i+k/ZvleSDIAiChUCON9SXgWWBFwBLp//LAF9qefwXcc+pYkJ8\nZeABM3s8fZ8DrJY+rwbcCZD2P5jkgyAIgoVAzpzFtsDaZlbksviTpD2BW5sOlLQ9cJ+ZXS9p83w1\ne5Y7E5gJYGYMDg4CnePtBcW+KnWyveQnSraXfD/I9pK/942vqD1u+jk/71GKM23atJ6/P1my/aJH\nP8j2ix6Lmmy/6DGR59d1fIbso/iq7TtK2waB/7Q4dlNgB0mvwz2oVgCOB1aSNC31HlbHh7dI/9cA\n5kiaBqyIT3R3YGazgFnp69C8efN6KjDSvrHKT2XZ8S57cHCwdXkTJdsvevSDbL/osajJ9ose4yE7\nY8aMVsfnGIuTgB9JOhY3GM8EPgSc2HSgmR0MHAyQehYHmNnbJX0HeDPuEbU7cF465Pz0/Rdp/2WR\ndCkIgmDhkWMsPoO3+N8OzADuAj5vZtWYUTkcCJwp6dPArxiOP3Uy8A1JtwD/AHYZw28EQRAEYyTH\nWBwPnGlmWxcbJL1C0hfN7INtCzGzK4Ar0ufbgJfVyDwKvCVDt6CPifUbQbDok2MsdgUOqGy7HjgX\naG0sgmAkwrAEQX+SYyyGgMUr2xZnDMEIg2CsRATeIJgccozFT4EjJH3UzOZLWgz4ZNoeBH1PBFUM\ngtGTYyw+APwAuFvSHcD/AHcDb5gIxYIgCIL+ISc21BzgJXgoji/goTk2TNuDIAiCKUxWPgszmw9c\nnf6CIAiCJwljTX4UBFOS8MoKgk7CkykIgiBoJIxFEARB0EgYiyAIgqCRmLMIgjGSM78RcyHBokr0\nLIIgCIJGwlgEQRAEjYSxCIIgCBqJOYsg6FNifiPoJ6JnEQRBEDQSxiIIgiBoZFKGoSQ9BfgJsFT6\nzbPN7DBJa+H5t1fGEym908z+K2kp4HRgQ+DvwFvN7PbJ0DUIgiDoZrJ6Fv8BtjSzDYAXAdtK2gQ4\nCjjOzNYB7gf2SvJ7Afen7ccluSAIgmAhMSnGwsyGzOzh9HWJ9DcEbAmcnbbPxsOeg4dBn50+nw1s\nJWlgMnQNgiAIupk0byhJi+NDTesAXwFuBR4ws8eTyBxgtfR5NeBOADN7XNKD+FDVvEqZM4GZSY7B\nwUGg03OkoNhXpU62l/xEyfaS7wfZXvL9INtL/sl6LQqmTZvWKDNa+ZDtLz0m8vy6jh/1kZmY2RPA\niyStBJwDrD8OZc4CZqWvQ/PmzespO9K+scpPZdl+0aMfZPtFjybZwcHBrPJy5EO2v/QYD9kZM2a0\nOn7SvaHM7AHgcuDlwEqSCoO1OjA3fZ4LrAGQ9q+IT3QHQRAEC4HJ8oZ6OvCYmT0gaWlgG3zS+nLg\nzbhH1O7AeemQ89P3X6T9l5nZ0GToGgSLKsUivljAF0wEk9WzWBW4XNJvgGuBH5nZD4ADgf0l3YLP\nSZyc5E8GVk7b9wcOmiQ9gyAIghompWdhZr8BXlyz/TbgZTXbHwXeMgmqBcGTkrpQItELCUYiYkMF\nQTAiYVgCiHAfQRAEQQuiZxEEwbgRkXKnLtGzCIIgCBoJYxEEQRA0EsNQQRAsFGLIatEiehZBEARB\nI2EsgiAIgkZiGCoIgr4nhqwWPtGzCIIgCBqJnkUQBFOOnKCKo5Etyz9ZejfRswiCIAgaiZ5FEATB\nBDDV5lnCWARBECxkcgzLwjJCYSyCIAimMOOVFCvmLIIgCIJGJiut6hrA6cB0YAiYZWbHS3oacBaw\nJnA7IDO7X9IAcDzwOuARYA8zu2EydA2CIAi6mayexePAh83sucAmwD6SnounS73UzNYFLmU4fep2\nwLrpbybwtUnSMwiCIKhhUoyFmd1d9AzM7J/AzcBqwI7A7CQ2G9gpfd4RON3MhszsamAlSatOhq5B\nEARBN5M+ZyFpTTwf9y+B6WZ2d9p1Dz5MBW5I7iwdNidtC4IgCBYCk+oNJWk54LvAB83sIUkL9pnZ\nkKShzPJm4sNUmBmDg4NA56x/QbGvSp1sL/mJku0l3w+yveT7QbaXfFyL/pLtJR/XYuJle8n3kh2J\nSTMWkpbADcW3zOx7afO9klY1s7vTMNN9aftcYI3S4aunbR2Y2SxgVvo6NG/evJ6/P9K+scpPZdl+\n0aMfZPtFj0VNtl/06AfZftGjLDtjxoxWx0yWN9QAcDJws5kdW9p1PrA7cGT6f15p+/slnQlsDDxY\nGq4KgiAIJpnJ6llsCrwT+K2kX6dtH8ONhEnaC7gDKMalLsDdZm/BXWf3nCQ9gyAIghomxViY2VXA\nQI/dW9XIDwH7TKhSQRAEQWtiBXcQBEHQSBiLIAiCoJEwFkEQBEEjYSyCIAiCRsJYBEEQBI2EsQiC\nIAgaCWMRBEEQNBLGIgiCIGgkjEUQBEHQSBiLIAiCoJEwFkEQBEEjYSyCIAiCRsJYBEEQBI2EsQiC\nIAgaCWMRBEEQNBLGIgiCIGhkstKqngJsD9xnZs9P254GnAWsCdwOyMzuTylYj8cz5T0C7GFmN0yG\nnkEQBEE9k9WzOA3YtrLtIOBSM1sXuDR9B9gOWDf9zQS+Nkk6BkEQBD2YFGNhZj8B/lHZvCMwO32e\nDexU2n66mQ2Z2dXASpJWnQw9gyAIgnoW5pzFdDO7O32+B5iePq8G3FmSm5O2BUEQBAuJSZmzaMLM\nhiQN5R4naSY+VIWZMTg4CMC9NbLFvip1sr3kJ0q2l3w/yPaS7wfZXvJxLfpLtpd8XIuJl+0l30t2\nJBamsbhX0qpmdncaZrovbZ8LrFGSWz1t68LMZgGz0tehefPm9fyxkfaNVX4qy/aLHv0g2y96LGqy\n/aJHP8j2ix5l2RkzZrQ6ZmEai/OB3YEj0//zStvfL+lMYGPgwdJwVRAEQbAQmCzX2TOAzYFBSXOA\nw3AjYZL2Au4AlMQvwN1mb8FdZ/ecDB2DIAiC3kyKsTCzXXvs2qpGdgjYZ2I1CoIgCHKIFdxBEARB\nI2EsgiAIgkbCWARBEASNhLEIgiAIGgljEQRBEDQSxiIIgiBoJIxFEARB0EgYiyAIgqCRMBZBEARB\nI2EsgiAIgkbCWARBEASNhLEIgiAIGgljEQRBEDQSxiIIgiBoJIxFEARB0EgYiyAIgqCRhZlWdUQk\nbQscDywOnGRmRy5klYIgCJ609GXPQtLiwFeA7YDnArtKeu7C1SoIguDJS18aC+BlwC1mdpuZ/Rc4\nE9hxIesUBEHwpKVfjcVqwJ2l73PStiAIgmAhMDA0NLSwdehC0puBbc1s7/T9ncDGZvb+itxMYCaA\nmW046YoGQRBMDQaaBPq1ZzEXWKP0ffW0rQMzm2VmG5nZRvjJdv1Jur7XvieTbL/o0Q+y/aJHP8j2\nix6Lmmy/6DGOso30qzfUtcC6ktbCjcQuwNsWrkpBEARPXvqyZ2FmjwPvBy4GbvZN9ruFq1UQBMGT\nl37tWWBmFwAXjENRs0K2r/ToB9l+0aMfZPtFj0VNtl/0mMjz66AvJ7iDIAiC/qIvh6GCIAiC/iKM\nRRAEQdBIGIsgCIKgkSljLCT9zwj7Nhun39hI0hsl7SBp/ZbHrCVp57bypeO26bH96ZJeLOmFkpbL\nKG+HHttXytErF0mLSVosfV5S0kskPa3hmKdKWiHjN5ZL5U7oueSQeW+6ng1Jr5L07PR5U0kHSHr9\nOOi1gqRn1Wx/4VjL7vF7Y3aiGc0zVFPGNpXvPeu+nOco5z6PlVTvPKWlbE+96u5/G/rWG2oUXCHp\nBOAYM3sCQNJ04BhgfWCjQlDSfsA5ZnZnbUkVJL06lfMAsCHwM+Cpkh4D3lkuR9K5ZrZT+rwj8EXg\nCuBzkj5nZqe1PJ+TgQUGMAVS/BKwZtr+K2AVSVcCHzCzB0uyO1fKGgC+Ury4Zva90r55kq4AzgC+\na2YPtNSvEUk7AV8H5kv6X+BjwMPAsyW918y+X5KdARyJxwBbDpgrCeAU4DNm9lhJ9qtm9r70+ZXA\nt4FbgXUkvSd50i1sfk/p/jVwCZ33+ot4fLRpki4GtgIuBD4kaXMz+0hbJSQtZ2YPp8/Cn8f7JC0B\n7GFm1ybR04CX1By/Pn5PinA7c4Hzzezmksz3gfeb2R2VY7dOv/f8mnIXAzCz+ZKWTDK3m9k/KnKt\nn6EGOt4n4Lp0/C8rv7d3+o21W5Zbe59TZb0tvrj4CeBPwCVmNr8itz5wHDAf2A/4OLBTkt+9fJ2B\ns4B/SboQf18vLuq6Gm6UdLCZWem3ngIciq9bW6fl+S1gKhmLDfHK5teSPgC8ANgf+DywW0X2COAg\nSbfiF/07Zva3Ecr+IvAaM/tbWih4rJltmlorJwOvKck+s/T5QGBLM/uLpEHgUvylBEDS+T1+bwBY\nubLtFPzh+aOklwH7mNnGkt6ddHhzSfYsfI3KfQyvzlwWeAMwBJSNxc3p/HYFPi/pKvyanGdm/65T\nTtLawM50vgjfNrOHKqKHARsASwM3Ai9N+j8T+C5QftG/CXzKzHZLxm4z/ME+GI9APLMku0np8xHA\nTmZ2Q9LLqHG5lvRa/CUsV3rnmdlFFbn3A2ea2TxJ6+DX/YXAH4G9zey3Jdn9664Pfs07WnaSvjSC\nbLUluw1eeS6d9FzNzB6RdCTeSGhtLOiszD4GbGhmd6dn6BupQjmHmlW8kg7En4szgWvS5tWBMySd\nWUobcCZwuaST8fft6fgz9Uxg95pycwxA62co833aD5gl6Rr8PX0m8FU8Dt2rKvq2vs9JXsABwG+A\nLYCfAxvj79fby88Q7s76hVTOZUmXPYHtgf/DGwoFfwC2xN/1DwOnSjoHOMPMrqyo8Rrg/5Lxex/w\nPOBo4FzgRT3OZ0SmjLEws/uB9yRD8WPgLmATM5tTI34bbly2Bt4KHC5fCn8G8D0z+2dFfvGSMfkr\nySCY2Y9SK7BM2Rd5mpn9JcnOkzS/IrsZ8A78RSkzgLcsyyxtZn9MZV2TelGY2Yk1D/MrcMN5rZl9\nDSC1SPesXgjgMTP7AfADSUvjBmUXvCdysZl1rJxPvbLtgZ8AL8UrrzWAqyW9z8yuKMub2T3puL+W\n9L+jZhhg5eJYM/uepEPM7F/AoZL+UKN3wQpmdkM67ra64YV0j9YDTscrA/BKbz9J25nZB0ri7zWz\n/0ufjweOM7NzJG0OnABsWpL9LP6iP16jV1WPPfEX/D81srtWvg+Z2VDpeSmeqfk15eZUZoub2d2w\n4BnaAr/va9D53BbsBTyv3KtLv3cs8Dv8GcPMviXpB7ihuBlYAvgMcKKZ1ZWb04jIeYZav09mdpWk\nDYFP4r3Sh4G9zOySGn1z7jN4I2eTZOAHgW+Z2WvTUN/X8fezYPnCOEo6wszOTNu/L+nwSrlDqZ47\nEThR0jMAAUdKWt3MFoRIMrNbge0kfQQ3MvcArx3L4uYpYyzSOONRuAXfFngdcKGkD5jZZRXxodQd\nvAS4JHXJt8Nf2qPxllGZ61Kr6TJgB3xYCUnL4MmZymwg6SH8AV1K0qqpJbdkjezVwCM1rQIk/bGy\n6VZJH0867Az8OsktQeWBNbNrU69nX0mX462VXgtqBkrH/RtvmZukFfGWeJV3Ay8ysydSpXGBmW0u\n6evAecCLK+exWLrW7yptWxxYslLu3yS9A7g8nd/tSXagen7A+pJ+k3RfU9JTzez+VHlUywV4nZmt\nV90o6Sy8V1Q2FuV3YpXU6sbMrpC0fKWIG4Bzzez6mrL3rmy6FrjJzH5eI/vJyqYfSvop8BTgJPx+\nXA28GjfSVdpWZv+U9KxUkZCey83x1ubzao6dD8wA7qhsXzXtK/NcvEK+Bh/ynY5fy8eoIcMA5DxD\nOe8TeAt9V+BreG/urZKuqw6FkXefwZ/Lolf+L2CVdH6/UfdcXLlOOLayr3p+Hb2/dA2/BHwpGdqy\nXtPwHmjRs3hdkntfcb1zmTLGAr+hX8WHZx7HjcCLgK9KusPMyq236kV/DDgfOD8ZgCrvwSvJl+O9\nllPS9iHgtZWyqgahYJlUTll2u14nY2avqmx6F95dPxhvjRUV3DLUdPXTy3W8pO/gQwK9+FaP338Q\nmN3jmGn48NNSpJarmf01Ga4yM/EH/lEzu6a0fQ1Sq7TEu3BDfRBuCIsIw0/Dz7nMcyrfHy7JfqJG\n30clvbQ0Pl/wUuDRyrazJZ0GfAo4R9IHgXPw7v9fK7J7An+v+T0ozZEl3lzzWwCY2VqV7wdKejne\nqLlaPiH5RtxwnF1TRNvK7L10Nyz+Kc9KqZpyPwhcKunPDKcM+B98vHtBBOjUkHox8D4z+4WkZYHD\n8XHzD9a11jMMQOtnKOd9kvRj/H5snYaJDwX2Aa6VdJSZlVc77wlUDUhB9T6DD4NeJOkneMP1O+k3\nn0b3cN9XlOaVzOyrJf3WweuaMh8a4fyqBv3XeKP2JeldniVpe7yO+66ZfaxXWb2YMiu4UzesbsgJ\nSe82sxNL39czsz+N8fdeUgx/jCeSdjCzXmOvYyl3upndOw7lfAAfnvgl3u0/ysxOlfR0fIK8auQW\nOpJegrcel2d4GGoN4EG8cXF9RX4PvGJ9Fm4Q78Rb30dZyZGgx2+tYmb3jesJNCD3mvqH1cy71d13\nuePHgrmbkZ6L1NJ/GZ1zPddaaWJV0oeAL1llslXSC4Cvmtlmle0vBX5rZo9Wtq8JvNLMvjnS+eYg\n6Wk1PQUkvbHoNVa2PwN3knn7GH/3dXhv60Yz+1HathiwhJnVDUXmlv9U4Anrnics9m/Yo/GwNHCo\nmR2S+5tTxljkopbeGEm26iUygA+5vAEYKBuNNP77BfzluhD4QjHmq5KnVPpe67WEdxs7vJYkfQ8f\nzz03jeWPdG51boU34K2/gfI5Snqhmf0mfV4CH7J6GXAT8Gkze6Sm/OfhrfubzGyk+YSRdLxwpJZg\nRfYTZvap0vcV8N7G6sCFZvbt0r4FnlI15TyDzkryntHoXiqvep0HgOupv86tdZb0LjM7JX1eHe/h\nvQSfD9hjtA2d1NM+AViR4ZD/q+Nefu81s1/VHFOM95eNxTU95iJGjaSVzayrlybpBtwh44xi+GyE\nMjbFe19Fj+XTuFfTkoDM7BeI2pMeAAAgAElEQVQNx9c2AFNdsQfwJvx6FU4dJ1hljm6EsmsbgXKn\njEPxOdYjcc+ol+P3+iNmdntJtstjMO3q8hjsoUOt4WzLlBmGkk8ezWK4kj4wTQYh6Roze1lJNtcd\n7zp8PLTcIlgZH2McwocoCk7BK/Wr8Rb4lZLekF6EjnFF8ryWNsZfgi+nLvQZwA/N085WmUf3OPNq\nuMEYotMt8DSGXSaPTOd1DD5fcQLdnmTgHjbLAc+Vu/TWViA1RrZggDyPjL3xYaGCU4E/49f5XZLe\nBLwttdg2qTkeWDDGe4/crXE9SY9ajauwWriLJnKuc47O72d4qPNY/DnZJun0NTo9ZJDPLx2M37NV\n0m/fhzdojiyd42nAe6zbXXSTtG+DyvbX4EO7f6bTuKyTxr4vSXLPwCet5+PDgPviFevNuFv33ZVy\njwSONnf62AifJ5ufGiu7VeYcnop7i10u6R78uT/LzO6im+Pw4bTlgB/iXnJXpefwy5ScE3o1ACV1\nNQBxb8M7gM/hw4kPAT/FnS9eYGZfrpxfjuv6aemcVsTrjFPxZ/01+DNQrltaewz2MpypYdxoOOuY\nMsYCf6g/iV/wvYGrkjW/FffOKJPljQG8BXe1+7yZXQgg6S9mtkWNHk83sxPS533lk7Y/kS+Kq7bG\ncryW7jOzN6cW6o74HMosuRfKGZVx4Y/glctHLLnpJX3X6iq1cwx1q3QtHkvjrTdWhdtWIIlrgSup\nccuk4i4qdwqoYwC/T2WeZWZvSp/PlXQIcJl6LzxsvS5D7d1FIe86Z+lcYj0zK+YTzpFUNydjuOPD\n5jY8cfwMfC7LGHbtXrZqKADM50WWrSn3eHxM//byRrn7+AUMzx2dhlfOy+IOCt/CJ1SLBseOlXJf\nb2YHpc9fAN5q7pSxHn5vyvMA95vZAcAB8sW1uwI3SLoZf+7LcwtLlO7D38zsqnR+N6ThlzI5DcAN\nS+/kVZKuNrNPpHfk17ghKlM0AsvDgr0agcuX3v33mdkxafvJcjfuMjkeg60NZ1umkrFY3oZ95o+W\nu8JeJE/J2tVltgxvDDP7rnxx1BGS3oW7QPbqhi8h6SnFeKyZfTO1iC7GH5hyuTleS0PpmIeAb+A+\n8ivjhuwg3LOrKPcYuafPcZLuxI1jr3JXlPRGfOJzqaIra+66WXdM2woEvGX5HjP7c7WQpFeZB3BD\n1TV+XiO7lIYnSDGzz0iai3sK1a1czVmX0cpdNP1uznXO0Xl1+bqMAeDpkpYo6VNt+ACsaWZHlTek\n5/uo9LwWXCjph7gLcXFN18B7jx3rTRLTGJ7jKTO3osf0onWdKrxCly9L2quuXEnTzB1RlrbkeGBm\nf5K0VI18cU4/BX4qaV+S9xKdYbfL727VKaI6cZ7TAHxMyYssVbb/Tfr8p8c7UjQCr2nRCJyfjOSK\nwDKSNjKz6+QT3FVnmRyPwRzD2YqpZCyQtKKlCUgzuzx19b+Le8lUZdt6Y5DKexhfQftifAy56kZZ\ncBI+ZHRl6dgfS3oL7odeLbet11LVd5w0tHVC+qvumwO8JbVcf4R7TdVxJe4ODL5WYrqZ3ZtapvNq\n5NtWIOA9vV5hFfatfD8dH6arm2z9duX79/GW3wJvETM7LRnlaiuvStO6jBx30ZzrnKNzedHddbgx\nuT/dkzrnhzskfRSYXRhb+ST2HgwbBcxsP0nb0T3E9hWrX/V+Cu4ddCadxmUXfGimoHwNT6+UUXf/\nvwpckIajLpJ0PN7a3pLkEl6ia37GfCL9IroN3MclLWNmj5jZucVGuTfZ6ZUychqAH8GHwf6DP/+7\npHKfDvygRr+cRuBH8WdjPt4TO1jSBsAK+OhBmRyPwRzD2YopM8Et6W3AbWZ2dWX7/wAfN7N3l7aN\nyRsjWfLlrHvxXl+SWhLPMrObxqGsg/HubV0FYmb2ubH+xngj6RHgFtK6DOB/bHhdxm/M7Pkl2W3x\nlbO17qJWWfFd+Z1xu845yD1jDsKNwPS0+R7csBxlY5jUlM9J7UD3/M3vSzKfwlvoD1eOXQefMylH\nFyj2bY57nK2HV8CFx9mp1V7dRJMagMcCzzez6hqrQmYAHwaqa0CNVPZq+JDQRmbWKoSIfCHf/dY7\nlEebMnYAfmwVB5VkON9kZl0N1yamjLGYSJThsZDkX4uPc19qnd4MC7xc0ve6icGd8RWXdRODrSZe\n5V4670/6noy3LF6R9P2spYn/kvwK+FzLrZXtCzylKtuf00OP31dlK8e9kuRpZfUrZavyn7Ue/uDp\nWqwG/LJcSUnatlqhq7JgCbjbzP6bXspXWeeEYyt30bGeX+61KB3X4RmWQ+o5782wR9bPS/sONbNP\nj3Ds0wDGYnjGC0mXmdmWNdtH7bWUjMHy1sMVtUb+dDOrc/4YM73KLtVDc/EFyCPWQzXHj8mte8oY\ni/Sg7I57K4z4oCjT9VI+kVV4LLwDn9A7C584fHv5wZX0OXzy6AZ8QuuLpfHcG8zsJSXZixieGHwb\nPjH4bbw7urWZ7ViSLU+8lkNW7ILHMjqyJHsB8Fu8K/uc9Nnwcd4NKuUuCC6HDyMtCC5X1TcXlbzQ\n5DGs9sEXuL0G+H5F52rspAHgnaThAzPbryS7XyrrZtyr6gNmdl6OzpIG27QSU+t4A+DmqjHMPL/W\nsg36/NXM6gLXNca+knQSPkx2DX5trzSz/dO+ruuWeuWfx4eHHsTvyQr4ZPpBReUkaeN0fR5KvauD\ncA+73+ONkxHXplR+c08zO7X0vdpYGcB7I8U84wtLsqfiw4c/ptNr6cB0Lb5cku24/2kuoHAZ7whT\nou6YUwN4zKfLkg4dTgoVo3yRmf2stK/DKOeUXVMPncqwA0O1Hmrt1t2WqTRnkePelut6WfVYOLr4\nTXV7LGwPvNjMHpeHcfi2pLXN7EN0ewXlTAy2nngFZpjZ61JraY6ZbZ62/1RSdUw4N7hc4ff+bTO7\nrbq/QnkOYyawjXkwxqNxT5Syzm/E508uKf3uLvgDXuXdSeeH09Dh2ZLWNLPje+i8HT5OPhfvvX0T\neIp8MnV3M7u0JHs58BZzt8534lFAfwJ8UtKsynOUc36tZZXnGYbax756WVG5Svo/PLrB9/BGSJ3H\n2ll4Q+LtNhzJeXF8cvhMht+VUxh2uz0eeARv+W6Fv2tVV9KRODwdU3A7/i5/Gg+hMYC/12+oOTbH\na+kSksu4fPX2ZnhDbXu8gVVeLb06bvhOwuceBnCPrWOo5+sMG+UvSVpglPFrUe7B5ZSd4zmV49bd\niqlkLHIelFw3xhyPhcLLAzN7QO63PUs+gV2dWMqZGMyZeF0sjWMvDyyXKtLb5d5TVR1yg8sVfu9X\nqNnvvdBjMbw187f0O/+SVI1j9FzcU2lb4AAzu0vSYWZWF3JksWLoKZ3X5rjBeCb1ld7ncFfOlfBW\n5+vN3UWfg/fmyi3qp5danPsBLzezv8vDwFxN53OUc345sjmeYdA+9tWCe5+e0ZlyV9zLqPciGzSz\ns8obktE4U9IR5XMrnnl8bL64nlfVNE7qegsFAwzPuRS/t4PcW28WvjbjfEmPWXd4C8jzWio/JzsD\nm6V78W28Qi2zEX4ND8GHe34t6d9WE4MqkWOUc8rOqYdy3LpbMWWSH5EeFIDqg0J3pbeUSl4wZvYZ\nPJLjT+gOZQzDHgunM+yxcAseevjjFdlb5fkvirKfMLO98G5zNabReUpJSszs0GJjuvlVL5AiTs+F\nkmalv4vwsOcfqMh+Dp/3uBb3oDhJ0o/wkMlVj6t/qpQMJRmOLfA5ibrgcveb2QFpKOTDwLq43/vl\nkmZWZFfEewbXAU+TtGo6v+Xojs/1TzP7IN6i+pakA+j9fN4rX41cHPsw3iIcxEPTV5lvZjebL0R6\nxJIThPlcT/U3HpNPSoJ7oBWr5f9D9wvZ+vwyZQvPsDqqnmGQYl/VbK/GvrpOPoG/APP5j1Pxif8q\n10v6qqSNJc1IfxtL+ioebbjgJklFQ+1G+UI7UsVWN1k9HXfXfUPNX9cq7tTL3Q7YXNJ59PbmKbyW\nilGDjyQ96ryWlpYnEdsQbzD9K/3WY/gQdvn355vZcXiMqEOSARipod1hlM1sJt5g7TLKmWW3rodS\nr2Nv4BOSjpUHwRzTnMNU6lnkuLdluV6mYYpnlzZdpd4eC2+pU87MDpX0tcq2ugVWmNktdOanwMwu\nSi9f48SrmZ0hyfAW7OPpBXsRHuKiY9Kc+uByD6l3cLmy3Ih+72a2Zo9D5+PDTnVlXi9pSzzkyVU9\njt+NSoTV1LLdTR79tsoDkt6Dj7ffL49lZHiI+qpL8ofwIJTfxYf3LpO7WL6SzuGRrPPLlD20hyxm\ndmDN5j2Ar6UKoRr7ao/Sse/oUeZJ+DBIld3w4c/DGX7m5uDvT9l1dm/c/ftQfPjjF6kHdGfaV+UH\nuDdhXa/jih46/gvYX+5W+vIeMpel3mWH11LqxX20In4Pw1Fe/6Hh6NArUx+9t+wm/Xp8aKwX16ni\naGFmn5J0F74Cf1RlZ9ZDOW7drZgyE9wweve2UfzOWvhE0e+sJtyvPPzCtnRW6hdbQxY6NXjIKCOe\n1Qi/sb5V4jlpeIFU0dJdH3dDrouTdaaZ7ZLxezkxuFrrUXPs+6wUtbOybw3ci2Q+XvHtileCd+BD\nXlVvshVxh4PCrXMOPkHaGAerQY+22eGWxPOMDKXvW5AmjC0tIOtRfmPsK7UPZZKN3HFkLdI1qxtG\nG0/qnuUamSyvJfmczFLW7XI65ndvhN/MeUdaZeCrOW7Mbt1Txlgk63mxtYzoqPbZ3lDvVKmb4t4e\np5Vkd8PdYS+hMxzGNsDhZnZ6SfYaa+9NsyCeFdARzwoPAtcqvaQq3jTyCKvH4N3/D+AxZv6CV5Qf\nNbMz2pTb47da65yjh7qT/Qzg3m2fBTCzal6ACSFHj8xrcSMeuuN+efKaN+KrzF8NXG/DoTLKujQ2\nUJThUVc6pnAD/7GV5gnU7QY+psp0JCM7wjHVZznHs6jWLbzH72S/e3JnkSHzBXrPxe/NH6yy+DHz\nuajLwLcYPvT6jur55NRxbZhKw1Ct89PKw2xvj3vfNGZ7IyNVKj5RtWG1FyGf3PwlnRPZOd40Oekl\nc1J4fhh/MJdP5b7YfIJwOt51bW0sVHF7zNE5U4/D8crzdwyP9y9O71X15Uqvae1LThTeHD1yrsXi\nNrwW5q345Ou/5Sueb8BdU8vnVtdA2QL4rKRyAyXHow5Jn8WH324APiZpgRs4pWCHygzM2cvIynNE\nV41szrOc41n0K0m34YbzDBt5jVBWLDlJh+FzLNPkc4Ub4yE6DpL0YvM50tGUPVIGvhMoZeBTZkbL\nNkylCe4/4JOtP8ErnrsknaDSZHOJvYFtzf2dt8ZfoENw639cjXzPVKl0eyINUD+RNJ/uiczFJD01\njZN2eMhQM25qZvek3+6IZ0V9Cs+b8AnV8t91pIn/Ek+Y2bxU7sM2nEVtNEMI1TSQOTrn6PG8dPyy\neAj4w/Fx28PT5w5SpXcI3gK7VD7HUlB1OTyt9PlIfOX2MfjLXA2rkqVHxrV4SFKxqnwenjEPvHFX\n984WDZT3mtmn09//4hVlef6j8KirUhvKBJ9w3tLc8WBDPE1n8X6Un+WiwnsFHrdsNzPbCu95H1ZT\n7uF4BbocyWOPYSNbNbQ5z/JGad8hwIOpQvy3mV1p3d5Fv8F7bIvhCYFulHSQ3BW7i4x7Bz7fuCme\ny3sfPBbZEXiitLeOoewBemTgw+fjyrwb2C6jjmtkKvUsWuenTbTN9gZ5qVI/g3sHXUJnuIhtcNfQ\nMoWHzAAwVCq3zkMGtY9nlZPC86/yhYTLA3+QdAy+jmJroDoZnuX2mKlzaz3M7K/4xN2OwI9KFVgv\n3kD7tS+to/Dm6pFxLf4X9wi7EV8seV36/ReQhrhqdG7TQGmV+a5EazdwywjMiRvZY3Aje3hqKe9e\nZ2DJeJbTtT0u6XecpHvpXccNmY/fH4J7Ib0MH467Kp1DOU92zr0DeDyNajwi6dZi2Cf1DruMckbZ\nORn4IK+Oa2QqGYuqK2bP/LR4N/VaSQuyvcECz6muMVbLS5U6O42dvpbh8eMrgIOtEmbD8ryFclKU\ntk7hia8E3Qf3nDko6X0wnkJ0j5oipieZ+yvbB/Ax1NHqnKsHZnaePLfHJ6kPbliQs/YlNwpvWz1y\n0oP+Ru7+/Rp8zubGVO6HqsObiVYNFMvwqEvcKunVRas8yewl6dN4SI0F5FSmmUY251kutrfxWqrW\nF9cA10j6MN4jKJPzHAP8VymgId4jAxbMK1WNRc5zcaCGM/B9ylIGPnxdTjVqQVYd14apNMG9ec44\nnMYh29tEoDFms5po5PmWT7UU8riy79tm9rZJ1qcxZpE858cXqkMRqdL7mJktVtp2auXwg2w4Cu+3\n0vDKpKOGNL7yObFyA6WY4K4a9fIxPUOZpP1Lg7eIa/atZmZz0+dRB+aU59H4JLCxNaTkbXOv2yLp\nbVYK8zOeSFrKahxt0jzDqpYWyY3Tb/WsL8a7jpsyxqKK3I1vXdz1sucLMw6/81szq1sI1iirUpwY\nucfEufik9wCeEOaXJdnlcF/xIkjaf/EEPidYyRsrya6Pj0vOx1chfxxfxPMnPLzFzSXZZ+BBDIdo\nEcwwB+WlEm2th4ZjFm2Ft6pqYxaV5FtVeqM4vwW+9JJWwsNHF5PhH7LSfEvmtaiLbXU+NWl8S8c0\nuh2rdyiTjYFqKJOmcx/RbVU90qTmyufca3nO7xNpkS2zhw690qrmxpJbqUcPsO43c56LMaWNHStT\nZhhK0jeBD6YX4bX4Q/MnYF1JB5jZd0qyrVOwpm29YtsMAM8YrSydcWK+gFeKF6bx0y9S8m7Aw1Kc\ng7cehY/3nonHvlrPOqOzzkrlLYe/VAfiE4Xb4+G3y63j0+jOcvZ6emc5Q+3zMufE4MrRo23MooIn\nKK0kVsO6BbWPwvtZhnMqHI0v9HoDfl+/nnQfzbXIyeK2wO1YUpfbsaSy23FOKJMmLsGHulBemtRe\n8k/I5wCr8jn3+mu0zJZZY5AH6J1WNTeW3Dz54sIzgO82GI6csnPSxmbVcW2YMsYCj6ZavAiH4aGn\nb9ewe+t3SrI5KVjBH9hvUT+J+JQxyJaZUVRc5jGaqgHj1iz1II6VdK2ZHSEPs/B73F2xYHlL7oqS\njjCzM9P270uqTiJmZTlTXlrVnBhcOXq0jVlUcC2wOb56u7xuYX9JrzKzBclhpOEovKmyWxCFl858\n5VU2MrMiBMlxknav7M+5FjlZ3KC92/FjpZ5UUyiTHLfVnDSpufI59zonW2aOQc6NJXcz/gztCnxe\n0lX4PTivpnebU3ZO9rvcOq6RqWQsFpO0grnnwXx8YpTUeqmeZ1YKVtzN7mirWf0oaesxyK4tnwwf\nwFNpLmPDfvzVG/ovSa9MLYkdSJNU5ougqp4Q5Re/ukBtLMEMIS+tak4q0Rw9rpfHJ5pNZwKm3emM\nWVTQtG6hnEksJwrvKvI1AwPACpIGSr2rqs6tr4XlZXGD5HaMt2g73I7d9i2gdSiTxJ7pt+sWuu5a\n+pybJjVHPuteq322zByDnJvG9zEz+wEekHNpvLe5C/AVSRdb57zeaN+Rpux3uXVcI1NpncXheGyo\ndwE/A74jaXdJp1GTX1jumQD4Q4XPBXyD+gBuH6S3V0XVaylHdkfchfBo3OtjsaTbdLpjyPwv3qN4\nAJ+72C/JPh0fdijzFQ0HKFywKlY+ofnjimxOMEPIS6taxOBaQOodfZhuH/kcPXbDc3Qcjuc2vzh9\nvgnP01AlZ91CRxRefHHbofJFTtWX7ESG1wnMxgMZFvMv1bhHOdcCM3vY3LX3c4ycxheS27E8CN0f\nJB0jaVP54rAFcz3mDiCvSNsew922HwX2teGw+2UKt9XZ1T+gnCWySJO6JSlNqqRXp15sV/ynTPmc\ne30UlWCdadhwK9wNu7z9u/gw52skfUc+N9KrEs26d5QaFWb2b3N2xucXLh5D2R+XDxliDWlj0/ac\nOq6RKTXBLWldvMtVjulzrpldXJFrnYI16ESLZlrVF+IvSbFOYlO85fYC4FjrnFT8OfBOK81XyAP0\nnYt79tS1lCcUNWRxk8+x7INXdv+Hz2vtice++rSN0klB7n30qFXiJPWQ3ZzONKlz8Dm22jSp8nmj\n/63InwucUic/WtQiO5yG06o+z8xWGYffPKCH8Z00JqKOm1LGYqJIw1h74T2DYgXsXOA84OTyw50j\n2/Cbs8xDG5e3NWZDG40ObcstybdOq5pTdsb5tc5yVpJZnOF1C0Xl1BXcUR7V9F/mkX/L25fAPU6+\n1X1FOuRqU37mnF+S3QJvCZbj+pxU1WsEPbJSaNY9b/2AMlKJagzZ4VoY5Kx3JIeM537UaWPHgylj\nLEoVZNdFp7tC/x4+jnmeVZLM9yj7DNxtbzadAdh2B55mZm8dpWz14S4YAG40s9VLsr2yoe0G/NmG\ns6Hl6tC63Fwydc6RXZACVN1Zzuak4ZsJR3kpP3PO73O459yl+PP8F7xSeB8euLLsrNHrObqBSiWZ\n87w1IelCM9tuhP09jWba37aCzElpPJ/u5GCr49d7yMw6ssOpfbywcXtHqkY587nISRu7Ij6vsRMe\nFmQIjwZwHnBktZHUhqk0wf0NvII8nO4K8pt0xmTZGJ8E/7J85e0ZwA/NrG78EXzCs5qJbA4elKs6\nnp4j+zf85pcnTosAaNXu8OtqykXd2dBydcgpF+WlVc0pO0c2J8sZ6lwPsSI+5PBSatZDjIS619Tc\nTvuUnznnt33xO5LOxHNlf0TS2an871SKaZtCM+d5q3MvLRjA86MUcrVGs9heNppJvm0aWMhLadw6\nO5xaBklM5L4jIxnl11W25b7XRZKppmyghrvNb27DoViegdeHRd7uLKaSscipIO8zszensd4d8aBb\ns+Qrfc+w7lwS/5D0Ftxnej4s6BK+he6wFzmytwFbmYc/6EDd6TMflfRSG3bjLKhmQ8vVIadcyEur\nmlN2juzSaZx5MSpZziTVha0or4c4Bp/grV0PoYx1MpaX8jPn/OZreGXuDJJ3m3nI8roYQG0ryZzn\nDXyC+0rq4w6VXWdvp73RhLwKsnUqUTM7JpVxXDqfw+g9aZ0TLyz3Hckxyjll56SNXdOG3c9JcvcA\nR8mdgLKZSsYip4IcAs8Ih/dIviGP/PoWPC5R1Vjsgo+XflXS/Qz7mV+W9jXJrogvNKvKfhGvfLte\nXnzVapk9aJENbRT65pQLKa0qcICkzXAXyhsk3Ywb2lkl2Zyyc2TvJjPLWYmm9RBZ62TM7Bx5TKYj\n5OtBeqX83IP25/dZPIT2n0h5DWCB59uNFdmcSjLneQOfF3iPmf25uqNsXDKNJuRVkEUq0fkMpxLd\nAF/F3TVJa+2zw+XEC9uDvHckxyjnlJ2TDfQOSR8FZhc9Z7mX5R4MO6ZkMZXmLNbEK8gtceNQriAP\nshRWPMn+xBri0IzwOysDWItQBjmyGb/fmA1tNDq0LVel+YLStsVJaVVL3eRR6Zx7fjV61GU5m4Mb\nlwHca+hZNpyF7jeVuYXr8ZAodetk7rTu6MXl/Rvgq6Krocyzzy8NZawN3JIzvpwqyY/hLctqxIAs\nJL0Zj/lUlw1yJyu5b6Zty+KBC5+F9/Rr50BSq/hruDtwtYLcx8yub9CrZyrRilzP7HDKiBdW2tf2\n3u0DXGVmXYZd0r5WE1Ylo+wBWmQDlccKOwgfOVkFf/bvwcPGHGWjiK81ZYxFmfGupOXuZveZ2aPp\nZu1BCheBe9/0bM1qOAXr760mlo5GnyZxOXzc97ZqZZKrb3pQMbN7UitlMzyrV513U1Za1XTMRuXz\nq7sOuXpUjut5LdL+ak6Fr5onmXoGvihrt5LsZsAdPVqFG5nZdaXvrbOtlcugxbXIla0cV1tJjuU5\nzqGN0UxybSvIuqxzN1t9qJb1U5m/tJLziio5sTWKeGGjvR9tGGvZkrax4Si0E8KUMRZjfRE0Qq5e\nSTcBLzOPu38U3nI6l7SYxszKYZlzUrC2TpOoUlAxea7ub+OBBNfBhwouKMnm6PsevAUygPfM9sAn\nfl+JV6Qnj3TdRkKeeOoY3PFgQ3yx5FPxBWHvNLM7S7Kt9ci5FhOJfH6kVba1zGvRWnaE3+vyRsp5\nLkrHNFa+ozSarfLUq5R1Dh9WKrLObZPkP1OS3Q/vOd6MT8B/wMzOS/vqesR1jZM/mtnvKnLZ96Nt\nI3A87nUqp5pidmPcoD6UDONBDNeHn7W0yj2HqbSC+wKGz+dIfHXmL/Fx0PI4OpLOr/x9H9i5+F5T\n9mKloY2tAZnZN9PLtWFFti4F6574ytmqS+ehuLfC3vhLsIqZvR13Eay2yspBxY7Ag4htgedl/tQY\n9H0/noxmQzxOz45mtlf6vX2pQdL6krZKL0R5+7YV0S/i2bq2xh/Ux8xsUzz/QtUI5eiRcy2QtLHc\nmQFJS0s6XNL3JR2l0irXtH+w8v0dkr4kaaa6J5dzsq3lXIscWST9pvL3W+AVxfeSaM5zUVS+5+HX\n/6bU+CkoJ2H6laQ/SzoitfxHRJ4G9gY8Xtcy6W8LPLRHtcGWk3Xu3fjw106p7I/LUyhDZdI6NU5+\ngTvAvBcf83898D11xyLLvR/Ch7+3xZ/rl+KrzX8tXyA6qrJr6q1y/bVypdxTgOJeH4/Pmx6VttWF\ndmlkKhmLnBdhddx741jcqh+Dhy8oPle5Ux6aANzzYw0YHu6qkJuCtW2axDIrWIqKae6+Wr2POfo+\nZmaPpCG7W4uhAPNYSnWL29pWIOCeSn9Ln/9KMqSpu7xaRTZLjxJN1wLyXpwFzg3yNRzvxBd2bUN3\nnK0hM7vJzA4xs3XwymoV3K2xmggq51rkyILf498Awr183oB75BSfC3KeC2hf+WalKKV9GlhIWefS\nu92RdY7u92mxovdjvm5iczwVbDFfVSancZJ7P3IagTllb4Z77x1T81ddL7ZYaTRlIzP7oJldZZ6N\ncG1GwVQyFjkvQk6uXvlC4XoAABITSURBVPAQIh+X+zMvibcQLscXx1STz28g6SFJ/wReJGnVpEdd\nCtYiTeIheCU1UprE9UutxvXkE1iFx1fVeyNH3yENp1l8fbFR0lOofz5at97wdKAnS3o7PlR0RSp7\nmZprkaNHzrWAvBenuoZjZ/NYSG/DGyG9ZDGza8xsf/zZqwZ6y7kWObKY2Q74ItNZePTl23Hje4d1\neiTlPBfQvvLNMZqQl6f+v+m8oTnr3L2SFqz/SLpvj8fsquacyWmcZN0P8hqBOWVfDTxS1FOlvytI\nC0FL3CSPSA1wo3xOBLkb8qjCqUwl19m9gdPlPtMP4i/Cr3GPqI4XwfJy9ZLGDbeQh7lYD19FOgdP\nRzm/IpuTgjUnTeJzKt+L8NJPw5MFjUpfvEVYuBKXAwSujAczq9JRgchjAp0tT11bfdHfg1ccL8cr\npGKh0xA+jDBaPVpfi8RNkvY0s1NJL465r37di5OzhuMLNb+FuadVtdGRcy1yZIvfbHThzXwuIFW+\nZvbrdPzDkrZP+pQr35wUpZCXp/5VlrLOVXRcAl9gVmY3Kq7TqZGwm6SvV2SHJC1hHtmhqXGSez9y\ncmW3LttGWDFv3d6dewPHp97xPOAXcrfdO9O+bKbMBHdB6UUo4v/0ehHKx7we2NQ6EwiNp047mFnd\nXEhZ5ql4qOleEWvLsuOWXrJH+T31lXQZsH9RgaRt0/CH/O0jGMtx1aNGtiNeVGXfivjw02b4i/MS\n/KW5E9jPSi6OqaVd5m02vIbjYjOr5mYojpvoezJiWtWKbCtvpBblrI4PA3V5KUna1Mx+lj5npyjV\nKNLAlo5tlXpYI6SNlTvE3GXdnoGrAc8xs2p05ixKjcAbi0Zg6vkuYTUpV0f5G43ZQJPMWqT60FpG\nK6hjyhmL0TDSQ9Xi2Gqq1OoK4AE8hPj7AMzseyXZGfhk/I54mOvCXe8U4DPWGc8qK5XoROib5FtV\nIOl7TnrXnOu2HR7mei4+vvxNfMHcUqncS3uc+6hfnPSiP8VKazhy7knmtajN4sYIaVXTcaN2v6w+\nF5V9T2c4cN1tNkI8tRyjKV8kVnad7bofyks9PKa0sb0aJzn3boSyaw1c5nPRMxso0JENNEeHtkyl\nYaie1FSQvR6qT8oDfX25cnxOqtSz8Jj19zHc5VwWf9GH6Iyr/018+Gm39Bub4ZNjB+MVZTkKaOv0\nkhOoL2Y2R9L/KOUZlk9iboSvh/gZneSkd83R43N4jJ2V8K77683s6tSr/BY9stmlXlvHQilJy1mn\nS+iS+Hh2sWhvC3qnYM1J+ZlzLXLTqta6X0rqcL/MfC6KyvlLwJr4MNGv8IRPV+JuqQ8muS6jmQxz\nr5zoL8InelfEe/9F8q8HgPdVjGFO6uHWaWN7NU5SL7naSMq5d6hHruz0bFVzZeeU3TobaC8jK/fo\n6zCybZkyxiLzRcjNRZwTAuIVeG/hWhsOgLa51axsxldiXgH+cEo6JI2RHyqp2irMSS85Ufoi6SB8\nnPU/ko7G14n8DDhc0slmVvYYyknvmqPH/KLFJekRSzH7zezm1API4fekXNKJkVKwvtqG04FCfsrP\nttciN63qF4HXmC80XAvP0bGppG1w98siaFxuyt9T8NbtH1PlvI+ZbSzp3ancN5fKzcmJfhq+Hqaj\nwpK0Ce6dtkGP82xKPZyTNjancZJz7yAjV3Zm2TnZQHOMbCumjLEg70XIykVMRqpU81Wm2wD7ph7M\ngT10AvibPBfD5fjNvT2VOcDYUolOlL7grqTPxSfsbwfWTpXUsvi6lrKxaJ3eNVOPB+R+8ivglfqH\n8EiaW9PtQog89WkdA3SnrmxKwVo2Fjn3JOda5KZV7el+KY/wWpCT8hc85WkRbv0aSSekzydWrmlu\nTvRl61q2qXe4bGVzTurhnLSxOY2TnDTFkJcrO6fsw/HYUF9hOBvo+fgalZHyajQZ2VZMJWOR8yLk\n5iLOSZVaeG0cL/e2+mL3IQt4F55S9SA8xHARcvlpdLte7obn6zicStIhuhcGTZS+4JPw/5b0X9w9\n8O+pjH+pM98zpPSu5ilCm9K75uixOz5cNx9vNe+KtxLvoCa4HL7+4wvUBxmsGuWHJD0/PUdFCtZ/\nU5+CNeee5F6Lh4EPyT2zZjNyWtXrJJ2MD2PsQG/3y6znArhV0sdTuTuTUp7KXZzL1yI3J/qFkn6I\nhygvy+9Gd6W3Y+V7z9TDZnaFpFfgbs7L05k29g8V2ZzGSda9o/PaNOXKbl22mZmkX9GZDXQTPHpA\nNV1rjpFtxZSZ4FZGTJ+0bUX8oSp7Tp1XfaiCTuQ5zZfEu+yP4BXwRfg4+vJm1mUxFjZyX/99rSZA\nnSrBAZWRgnWySD3N5czsnz32L4Ebyefiep9iZk+kFuQq1jv6a9PvroQHJSzKPdLM/pnenecUw39p\nLH4varIn4onHurx/5E4KddkWJyVUS0WXGXjjZCOrJEgaZXk7AD+27oCWzwLeZGZ1EX7HFfk8Vpnr\nzV2fpwNvNrOv5JY5ZYzFRJJaaO/HWx5fxkMD7wz8AZ+gLk+QLoiTk17iAxlO+flp6/SmySk3J71k\n63Ibzrsutes0fCx6CDg7ndvb8OGPr6Q5l5HK/JPV5DLI0aPm/N6Kp5qsPT9Jzwb+bjWutZKmW8UL\nR+1TsGbpUfPbtdeidK/vwodKet7rHMbruZgI1CMNbOb79D18ruHciT6X3Oe41/nllK2MbKATwZQZ\nhsqseItctjvTGeirVy7b0/Du8tL4hNXN+LDGDnhX+J0V2cIb50jci+UY/AafgHe1R1tukV7y6vT9\nU3ildgqdHjKty1VeVq9ikdMZpU0/T39dyFexF62RYhJxmWK7ma1Qks3R4zTaXzesPsT2KmZ2X9VQ\nJPkngAvT30i01iPnWpB3r1H7TICt9U1lbZT2z8WHU07BK+o/ATPN7FdJLtd4193ra9KQWzVX9mm0\nf5+KDJhfUkMGzBzDknnv6s5voNf5ZZadkw10pPMZVb71KWMsyHsRTsbHt4+kM5ftoZJeYN3+2Ov5\ncKEG8MQ7W5vZkKSr6E5GU16huRXwUvPVvz+pkc0pNye9ZE65uak2yxXTSviL2ytF6am4e+tHbDgB\nS22ay0w9cs4v9+XNScGao0fOtci519A+E2DWdcPXshyW9P55Ov9tJG2V9r08yZ1GhhGifRpYyHuf\n7rP2GTBbGxby7l3u+eWU3TobaG4jsA1TKTbUemb2YTwy5fPwMeqf4pm2qq54G5rZJ83jA30Qdzv8\nEb7s/329fsDc9/6C9L/4Xh3HW1HSGyW9CU/E89gIsjnlzpe0ntz1bRkNx3rpSi+ZWe5teNCztUp/\na6eHtW7RWjlY4NEMV0zX4hVT+ff3w1dOnyFpv9Sj6zXumatH2/MDf3mvL/1dx/DLe11Ftnx+5Yq3\n6/xy9Mi8Ftn3usRGZnaoeVyo4/A1Etn6JpYwswvN7Ay8lXt2kr+UTg/DnHcPPOPbH4EdinuNL5Jc\ny7rnDHLep+J8HjKzb5jZ64D1cS+9gyqy95nZm9P1+T5uWOZKOlVSR37qzHuXdX6ZZf9D0ltUcg+X\ntJikt9KdDfRv+LNdfe6vo6YR2IapZCyA1i/CY2myCVVy2dbIgnubLJdkyrkgnoVHqy1zJd6i2h6P\nxzI9yT4Dr7RGW26RXnI2w+kl/4y3+D4+hnKLVJt1NE3EtamYrmc4AN+V1Pvz5+qRc36QVzmVaTq/\nLD0yrkXOvQZfKLe/PBbTCuoMpV5+x3Ov26OSXiNPVzwkqcjT8mp86LZ6fq2MkJkdg3v0fELSsfKU\nor0qyJz3qWs4ycz+bmYnWCW3B3mGJefe5Z5fTtm74CMh90j6U+pN3IP3IKsJybIbX01MmQluSSfh\nS+Gr46PPwvPQvrK0bUu867wgl62Z/VIe1uAjZvbRmvLXp97b4w/Fy1GSfU5JdohhT4+usACjLHcG\n3p28F/fg+u0Yy62V7aFv6xSllbJXx9dm3IKPE7e+FmO9bkl+dXyiuMhRfWOdoRjD+eVc5zbXIude\nH1bZNFImwBx9N8AN9Xzc3fy9+Pj4XODdZvbzJNf63avRvTEN7Hi8T1VZZaZWzrl3ozi/0TwXHRPc\nNeeXndq1iSljLGBUFe+b8NXd8xn5Afwo7vFzJp0TS7sAZ5rZkRXZXZPs3Bay/VBuK9kkn1MxHVjS\nedz0yNW5ov+IL2/m+eXonHstWt3rJjQcbXdM122kctP3LONdKWtBGtiacifkuR9J55qKt/W9G8X5\n5TwXBxbbm65Fzvm1ZcpMcFcelGvS5tXxiasz8cnssmxxg64uy8pzTFdv/t7A86zimiaP6/+7ctlT\nXBbzHBBdmKemvLyyea9+0Lmi5/mSfoSnFO2q9DLPL0ePiboWTRzO8ELTCSk3592rwzyRUbGYtqxv\nrs6tZUfSuaYOyLl3ueeXU3Zr2czza8WUMRZMbGUzHx8OqHo4rEp3ApapLNtE9UXoS50bXt6R6Mvz\nU2fq1DIDwPRJKDfrfcooN1fnHNkcnSfquuWWPVHn14qpZCwmsrL5IHCpfJKxnKxlHYZDdDwZZHNf\nhEVO50Xx/JJer6XbI2aAzjUwE1Vu7vvUttxcnXNkc3SeqOvWL+fXiikzZyFpWzykb+2FtOQ7nytb\nOmYxfEFSefzvWktRNp9EsvcywotgZjMWZZ0X0fM7GTjVUsC6yr5vm9nbJrLc3PcpR99R6NxKdhQ6\nT8j96Jfza8OUMRYwcS9jMEzui9APZFami9z59QOL4vu0KOqcw3if35QyFkEQBMHEMOUW5QVBEATj\nTxiLIAiCoJEwFkEwQUi6XfUZ6IJgkWMquc4GAZKuAd6BJ2U628xe0nBIXyNpD2DvhpAZVwDfNLOT\nJG2OZ7Yr8jw8gLtrfsHMrp1YbYOpTPQsgimDPDnOM3F3wQ3xqLLjVfai1LC6y8yWw1OLboLnlfip\nPLR4EIyKRekFCIImng/83jxHw/+3dz8hVtVhGMe/JLlIkUEXgjjuohBaDI4FomHbwIUgT4RMFCSJ\nIAoyiKAtRDQRRAIRTCsKEl8iCIxEQsJoocKgoLgR7WaMIjTOgGLUaC3e383jda7n4h90Zp4PDNzL\n+XPPncV5z/n97nneXmqKhaR/gXXkw04zyKezN0bE3XJFv4qMSngP2CfpYzJXahXZu+EoGcc9UvbX\nB2wDppNhhNXP+pJMut1c3i8l7wbmlvfdZFT1EvIi7hCwl2zw86Kkm8BoRHR1+s8omUx/kOmnM8kO\ni73KVNrdwEoy4bQBvBtj9K83a3KxsHFP0gdkouxU4AVJw+QJ+7ak7UBPRFxus/lyoLes/xMZZX6g\nLHuDzNaZTTa5f7/8vQVcB74iH3zqkzSfbPTzNhlzvYPM4unk+KcAR8jhoz4y/rs3Ii5IWk3NMFSH\nvgPWSJoGLAbeJNvGjpDR3MMP2dbMxcLGvxIE+IWkX4C1wBCZeNpTl3gK7IzslDckaQ8ZMNksFoNx\nL8p5VNJKYHdEXAKQtAk4V4rVCuBIRJwoy7YwdhTEWF4noxn6I9vWAjzwUOBjGiSfQu8C/iGHqF4F\nTj1qCqlNLp6zsHFN0kxJw5JGgEXAz+TdwSvADUnra3ZxpfK6QZ60x1oGD2btNMgLrtll2f/rR8Qt\n4M8Ov0Y30KgUiqeh2QtiOCKOk3dEe4HrkvYrW5GateViYeNaRAyVcfyPgAPl9VFgWUR0RcSeml10\nV17PI6/Am1rvSgbJCfTq+qNkY6Kr1X1JegmYVVn3FtnYpqnaS+MKMK/NJPqTilhYDgyUIkZEfBoR\nC4D55HBU/xP6HJugPAxlE0X11089ZM/hTvRLOknOWayjZWK6xSFgo6QfyR7H24HDETEq6VvgpKTF\n5KT4Vu6/GDsDbJC0jZxbqd7xnCKLzSfK5kt3yD7xv5KFaK6kqRHxd4ffCYAykT2HjKv+kGxPiqSF\n5dgGyCL2F4+YRGqTh+8sbKJYAAxImgXciYjW1Nh2vicLyxngB+DgQ9b9HPgaOAFcJk+yawEi4jzZ\nhvUb8sR/g3udzyjbnQV+A44Bh5sLSrDbMjIR9Pey3Ttl8XGy/8A1Sa09p9uZU349dRM4DbxG9mM+\nVpbPAD4rx9ggh8t2dbhvm6QcJGiTVvnp7MsRcfFZH4vZ8853FmZmVsvFwszMankYyszMavnOwszM\narlYmJlZLRcLMzOr5WJhZma1XCzMzKyWi4WZmdX6D3FcKKwNRlEIAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x5Har9SHWQ40"
      },
      "source": [
        "#### Since variables including helpful / total votes take place after a review is made, it hardly makes sense to use such variables in the predictive model. However, a first exploration might help us better understand our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C4NQ6YG3w8BQ",
        "outputId": "e56fee40-d8b3-4a0b-ed9d-3b538bf75d74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "hv = pd.Series(tr['helpful_votes']).value_counts()\n",
        "hv.iloc[:30].plot(kind='bar')\n",
        "plt.xlabel('# helpful votes')\n",
        "plt.ylabel('count')\n",
        "\n",
        "print(hv.iloc[:30], '\\n')\n",
        "print(\"Most reviews, i.e., {:.{prec}f} percent of them end up with less than 5 helpful votes.\".format(hv.iloc[:5].sum() / hv.sum() * 100, prec=1), '\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0     325796\n",
            "1     219490\n",
            "2     134844\n",
            "3      89246\n",
            "4      61159\n",
            "5      44587\n",
            "6      33889\n",
            "7      26028\n",
            "8      20402\n",
            "9      16568\n",
            "10     13874\n",
            "11     11720\n",
            "12      9789\n",
            "13      8476\n",
            "14      7243\n",
            "15      6433\n",
            "16      5415\n",
            "17      4750\n",
            "18      4230\n",
            "19      3850\n",
            "20      3442\n",
            "21      2965\n",
            "22      2770\n",
            "23      2592\n",
            "24      2295\n",
            "25      2026\n",
            "26      1952\n",
            "27      1791\n",
            "28      1575\n",
            "29      1533\n",
            "Name: helpful_votes, dtype: int64 \n",
            "\n",
            "Most reviews, i.e., 75.6 percent of them end up with less than 5 helpful votes. \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAEQCAYAAACAxhKnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnNjkZZImVgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6SOfNhdjzKdr",
        "outputId": "42bd367a-d2c1-49ac-d929-92040a06f9fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "tv = pd.Series(tr['total_votes']).value_counts()\n",
        "tv.iloc[:30].plot(kind='bar')\n",
        "plt.xlabel('# total votes')\n",
        "plt.ylabel('count')\n",
        "\n",
        "print(tv.iloc[:30], '\\n')\n",
        "print(\"Most reviews, i.e., {:.{prec}f} percent of them end up with less than 5 total votes.\".format(tv.iloc[:5].sum() / tv.sum() * 100, prec=1), '\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0     244560\n",
            "1     188252\n",
            "2     137370\n",
            "3      97229\n",
            "4      72455\n",
            "5      54591\n",
            "6      41680\n",
            "7      33119\n",
            "8      26725\n",
            "9      21555\n",
            "10     18245\n",
            "11     15174\n",
            "12     13232\n",
            "13     11251\n",
            "14      9750\n",
            "15      8751\n",
            "16      7497\n",
            "17      6784\n",
            "18      5943\n",
            "19      5387\n",
            "20      4854\n",
            "21      4440\n",
            "22      3920\n",
            "23      3696\n",
            "24      3355\n",
            "25      2994\n",
            "26      2863\n",
            "27      2630\n",
            "28      2400\n",
            "29      2238\n",
            "Name: total_votes, dtype: int64 \n",
            "\n",
            "Most reviews, i.e., 67.3 percent of them end up with less than 5 total votes. \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAEQCAYAAACAxhKnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iQKfmEev8nTP"
      },
      "source": [
        "### 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-I_mhQuV3-ap"
      },
      "source": [
        "#### Before the model is established, we need to preprocess the data and to make sure they are the right form as inputs feeded to the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WeO83UHVNK43",
        "colab": {}
      },
      "source": [
        "tr = tr.dropna()\n",
        "\n",
        "tr.star_rating = tr.star_rating.astype('category')\n",
        "\n",
        "tr.verified_purchase = tr.verified_purchase.astype('category')\n",
        "ts.verified_purchase = ts.verified_purchase.astype('category')\n",
        "\n",
        "# concatenate review headline and review body to a new variable \"review\"\n",
        "tr['review'] = tr['review_headline'].str.cat(tr[['review_body']], sep='. ')\n",
        "ts['review'] = ts['review_headline'].str.cat(ts[['review_body']], sep='. ')\n",
        "\n",
        "X = tr['review'].astype(str)\n",
        "y = tr['star_rating']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dZ9EeuSUbJ3j",
        "outputId": "d8257df5-e796-40ae-a0f5-dd30a8704f77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print('Review: ', X.values[5])\n",
        "print('Rating: ', y.iloc[5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review:  A \"SEMI-DIALOGUE\" BETWEEN TWO TOWERING 20TH CENTURY INTELLECTUAL FIGURES. Rudolf Bultmann (1884-1976) was a German theologian and professor of New Testament at the University of Marburg. He was the author of many influential books, such as [[ASIN:1565630416 The History of the Synoptic Tradition]], [[ASIN:0684143909 Jesus and the Word]], etc. Karl Jaspers (1883-1969) was a German psychiatrist and existentialist philosopher, who wrote many important books such as [[ASIN:B000S6THQE Reasons and Existenz]][[ASIN:0812210107 Philosophy of Existence]], [[ASIN:0300097352 Way to Wisdom: An Introduction to Philosophy]], [[ASIN:B0007EW69C Man in the modern age]], [[ASIN:0226394859 Future of Mankind]], etc.<br /><br />The translator notes, \\\\\"The essay 'Myth and Religion' by Dr. Jaspers ... was later published (in 1953)... Bultmann's reply first appeared (in 1954)... Dr. Jaspers' rejoinder was first published in an open letter...\\\\\"<br /><br />Jaspers argues, \\\\\"It we forget that myth is also a code language, a cipher, it loses all reference to transcendence, it becomes mere tangible presence.\\\\\" (Pg. 14) He suggests that Bultmann \\\\\"alternates between empirical, philological exegesis and a theological appropriation of religion. The two opposing goals, which are the historical investigation of religion and the primary comprehension of faith, do not add up to a convincing statement, but rather collapse for lack of tension and clarity.\\\\\" (Pg. 28)<br /><br />Bultmann replied, \\\\\"(Jaspers) is as convinced as I am that a corpse cannot come back to life or rise from the grave, that there are no demons and no magic causality. But how am I, in my capacity as pastor, to explain, in my sermons and classes, texts dealing with the Resurrection of Jesus in the flesh, with demons, or with magic causality? And how am I, in my capacity as theological scholar, to guide the pastor in his task by my interpretation?... When (Jaspers) says that the redemptive history... must 'be tested existentially and judged on the basis of ... the truth that arises from it in the reality of life,' I can only reply to such a vague statement by the question, 'Well, how is this done?'\\\\\" (Pg. 60-61)<br /><br />Jaspers then asserts in his own reply, \\\\\"you do not go far enough... I assay the differences between us in terms of the opposition between liberalism and orthodoxy, and I conclude that your position can be defined as orthodox. The actual meaning of my lecture, however, was to asert the rights of philosophy.\\\\\" (Pg. 72) He concludes, \\\\\"The greatness of Protestantism... achieves reality only in exceptional individuals... who translate into action the great moral earnestness... inherent in this faith... I see hope in the liberal faith... which as such is capable of transforming the Biblical faith in all its manifestations.\\\\\" (Pg. 114-115)<br /><br />Although one suspects that the two protagonists are often missing the other's point, this still a fascinating interplay between these two thinkers.a vague statement by the question, 'Well, how is this done?'\\\\\" (Pg. 60-61)    Jaspers then asserts in his own reply, \\\\\"you do not go far enough... I assay the differences between us in terms of the opposition between liberalism and orthodoxy, and I conclude that your position can be defined as orthodox. The actual meaning of my lecture, however, was to asert the rights of philosophy.\\\\\" (Pg. 72) He concludes, \\\\\"The greatness of Protestantism... achieves reality only in exceptional individuals... who translate into action the great moral earnestness... inherent in this faith... I see hope in the liberal faith... which as such is capable of transforming the Biblical faith in all its manifestations.\\\\\" (Pg. 114-115)    Although one suspects that the two protagonists are often missing the other's point, this still a fascinating interplay between these two thinkers.\n",
            "Rating:  5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R3MHvTtWc8hY"
      },
      "source": [
        "#### There are plenty of ways to preprocess, i.e. to tokenize the text data. We will firstly use the convienient one Keras provides. \n",
        "#### The function Tokenizer() takes care not only of splitting, lowercasing and stripping special characters, but also help to keep N most common words in the data set, preventing too large input vector spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K1hAHJfAc6Fu",
        "outputId": "cf1a2719-cc05-4533-ebc8-172b7a63d439",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_words = 16000 # We will keep only the 16k most common words\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words) \n",
        "tokenizer.fit_on_texts(X) \n",
        "sequences = tokenizer.texts_to_sequences(X) # list: string - numbers(indices)\n",
        "word_index = tokenizer.word_index # dict: word - number(index)\n",
        "\n",
        "print('Found {} unique tokens.'.format(len(word_index)))\n",
        "\n",
        "# word_index is a dictionary that maps the word to its corresponding index in the vocabulary:\n",
        "w1 = \"cat\"\n",
        "print(\"Index of\", w1, \"in the vocabulary: \", word_index[w1])\n",
        "\n",
        "# sequences is a tuple of lists denoting the corresponding indices of the words in each string (review):\n",
        "print('The indices of words in the number', sequences.index(sequences[4]),'review:', sequences[4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 726132 unique tokens.\n",
            "Index of cat in the vocabulary:  1580\n",
            "The indices of words in the number 4 review: [305, 52, 921, 42, 85, 9, 10, 6, 2656, 4, 111, 42, 35, 768, 8, 66, 8, 524, 20, 814, 41, 6, 130, 17, 35, 820, 6469, 8, 1, 929, 9289, 112, 89, 52, 463, 6, 23, 276, 36, 34, 399, 307, 1, 91, 3, 5542, 99, 6518, 63, 34, 175, 4, 78, 11, 2, 81, 1986, 1275, 254, 384, 7, 503, 5, 2096, 3340, 4, 249, 5, 51, 66, 20, 61, 11, 322, 4, 403, 56, 403, 425, 42, 4, 66, 84, 62, 4940, 13, 13, 2, 317, 138, 9, 10, 305, 52, 648, 5, 229, 305, 140, 6, 110, 41, 11, 322, 140, 4, 7, 9, 174, 5, 166, 3, 4029, 12, 6, 2, 74, 227, 358, 112, 783, 15, 41, 27, 300, 16, 1, 554, 2897, 3, 66, 4940, 5101, 88, 249, 5, 254, 36, 34, 74, 227, 23, 1983, 2869, 4, 78, 203, 176, 14, 376, 1, 1019, 49, 3, 1, 185, 40, 3780, 632, 582, 349, 6, 3780, 4, 1, 1829, 2, 268, 2836, 1, 1019, 1387, 3, 5, 443, 840, 40, 806, 282, 33, 2999, 13, 13, 38, 43, 18, 21, 358, 207, 14, 5, 354, 26, 19, 53, 4, 105, 1, 91, 3, 1, 6706, 1424, 6, 144, 78, 11, 64, 105, 11, 225, 20, 43, 18, 21, 207, 14, 169, 42, 4424, 12, 81, 9350, 8, 2, 382, 634, 4029, 6, 144, 207, 2100, 506]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6rN3VhHwg-aC",
        "outputId": "65599e4a-2a45-49bc-d2a1-afc8130b87f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Furthermore, we need to pad the sequences so that their lengths are the same and do not exceed a specific maximum length.\n",
        "maxlen = 256\n",
        "X = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "# Transform the target \"rating\" to one-hot encoding\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "y = np.asarray(y)\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "encoded = y.reshape(len(y), 1)\n",
        "Y = onehot_encoder.fit_transform(encoded)\n",
        "\n",
        "print('Shape of data tensor: ', X.shape)\n",
        "print('Shape of label tensor: ', Y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor:  (1099194, 256)\n",
            "Shape of label tensor:  (1099194, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eFljlxs0FAqr"
      },
      "source": [
        "### 3. Word Embedding using pre-trained GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QGugJCn95SiZ"
      },
      "source": [
        "#### The next step would be associating words with vectors, which the neural network expects as its input. The use of word embeddings, which are learnt from data and contain more information as well as lower dimensions than one-hot word vectors, is a powerful way to do that. One can either use the data to learn a task-specific word embeddings, or load pre-trained word embeddings that have been precomputed. In this task, we will load pre-trained word embeddings. GloVe pre-trained word embeddings with 200 dimensions are chosen as our try."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dho6hLqYiUw-",
        "outputId": "76798ba3-4b3b-498d-b33e-09fe138281ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Map words with their word representations from GloVe pre-trained word embeddings\n",
        "embeddings_index = {}\n",
        "gl_PATH = './gdrive/My Drive/DL/NLP/glove.6B.200d.txt' # pretrained GloVe\n",
        "f = open(gl_PATH)\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found {} word vectors.'.format(len(embeddings_index)))\n",
        "\n",
        "# Create a embedding matrix for the Embedding() layer in the model, with the shape of (max_words, embedding_dim).\n",
        "embedding_dim = 200\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items(): \n",
        "  if i < max_words:\n",
        "    embedding_vector = embeddings_index.get(word) # Convert word of index i in the word_index to its word vector with 100 dimensions\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wKFV7p9xd3tM",
        "colab": {}
      },
      "source": [
        "# Split data into training, validation and test data sets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# train : val : test = 98 : 1 : 1\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.02, random_state=1)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qlj-Sqdf4eLE"
      },
      "source": [
        "### 4. Building Models and Training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZtoMIKOqdioz"
      },
      "source": [
        "### Model 0 (Baseline Model): Pre-trained 200D GloVe embedding layer with fixed weights + 1D convolutional layer + 2 GRUs, with layer normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xlY064IgooKn",
        "outputId": "0546d73d-0dd8-4c00-ec4c-337234de4421",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!pip install keras-layer-normalization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-layer-normalization\n",
            "  Downloading https://files.pythonhosted.org/packages/95/76/42878fe46bff8458d8aa1da50bfdf705d632d33dbb7b60db52a06faf2dad/keras-layer-normalization-0.12.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-layer-normalization) (1.16.4)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-layer-normalization) (2.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-layer-normalization) (2.8.0)\n",
            "Building wheels for collected packages: keras-layer-normalization\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/9b/9e/f4072915f660e90bb3638332276f4de80476f3afcb5d010d6f\n",
            "Successfully built keras-layer-normalization\n",
            "Installing collected packages: keras-layer-normalization\n",
            "Successfully installed keras-layer-normalization-0.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rdT4CZslBAK7"
      },
      "source": [
        "#### Study finds that layer normalization works better for RNN architectures, since batch normalization requires sufficient amount of data points with in each batch in order to get a reliable estimate of the mean and the variance of the data. Instead, layer normalization can be Implemented on each single data point. Ba et al. (2016) find that LN accelerates the training speed of RNN models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IoV0lJnwtK8N",
        "outputId": "989777f5-b34b-451e-cb5f-d578baa61a6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Embedding, LSTM, Dense, Input, Dropout, GRU, Conv1D, MaxPooling1D, BatchNormalization, Activation, concatenate\n",
        "\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from keras_layer_normalization import LayerNormalization\n",
        "\n",
        "inputs = Input(shape=(256,), dtype='int32')\n",
        "x = Embedding(max_words, embedding_dim, input_length=maxlen)(inputs)\n",
        "\n",
        "x = Conv1D(128, \n",
        "           7,\n",
        "           kernel_regularizer=l2(0.01),\n",
        "           kernel_initializer=keras.initializers.he_normal(seed=42))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling1D(5)(x)\n",
        "cnv = Dropout(0.2)(x)\n",
        "\n",
        "x = GRU(64,\n",
        "        return_sequences=True, # to make sure the input in the next recurrent layer has the same dimensions as the original input           \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(cnv)\n",
        "x = LayerNormalization()(x)\n",
        "x = GRU(64,\n",
        "        return_sequences=False,           \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "\n",
        "outputs = Dense(5, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# load the pre-trained embeddings into the first Embedding layer\n",
        "model.layers[1].set_weights([embedding_matrix])\n",
        "\n",
        "# set the Embedding layer to be non-trainable, preventing the pre-trained embeddings updating in the training process\n",
        "model.layers[1].trainable = False\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.0002),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "\n",
        "cb = EarlyStopping(monitor='val_loss', \n",
        "                   mode='min', \n",
        "                   verbose=0, \n",
        "                   patience=5,\n",
        "                   restore_best_weights=True)\n",
        "\n",
        "history = model.fit(x=X_train, \n",
        "                    y=y_train,\n",
        "                    validation_data=[X_val, y_val],\n",
        "                    epochs=99, \n",
        "                    batch_size=2000,\n",
        "                    callbacks=[cb])\n",
        "\n",
        "score = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(\"Accuracy in the val. set:\", score[1])\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy in the test set:\", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "embedding_4 (Embedding)      (None, 256, 200)          3200000   \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 250, 128)          179328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 250, 128)          512       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 250, 128)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 50, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 50, 128)           0         \n",
            "_________________________________________________________________\n",
            "gru_13 (GRU)                 (None, 50, 64)            37056     \n",
            "_________________________________________________________________\n",
            "layer_normalization_14 (Laye (None, 50, 64)            128       \n",
            "_________________________________________________________________\n",
            "gru_14 (GRU)                 (None, 64)                24768     \n",
            "_________________________________________________________________\n",
            "layer_normalization_15 (Laye (None, 64)                128       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 3,442,245\n",
            "Trainable params: 241,989\n",
            "Non-trainable params: 3,200,256\n",
            "_________________________________________________________________\n",
            "Train on 1077210 samples, validate on 10992 samples\n",
            "Epoch 1/99\n",
            "1077210/1077210 [==============================] - 172s 159us/step - loss: 1.8303 - acc: 0.6262 - val_loss: 0.9768 - val_acc: 0.6766\n",
            "Epoch 2/99\n",
            "1077210/1077210 [==============================] - 164s 152us/step - loss: 0.8960 - acc: 0.6775 - val_loss: 0.8048 - val_acc: 0.7023\n",
            "Epoch 3/99\n",
            "1077210/1077210 [==============================] - 164s 153us/step - loss: 0.8309 - acc: 0.6881 - val_loss: 0.7682 - val_acc: 0.7089\n",
            "Epoch 4/99\n",
            "1077210/1077210 [==============================] - 165s 153us/step - loss: 0.8062 - acc: 0.6942 - val_loss: 0.7675 - val_acc: 0.7125\n",
            "Epoch 5/99\n",
            "1077210/1077210 [==============================] - 164s 153us/step - loss: 0.7943 - acc: 0.6968 - val_loss: 0.7441 - val_acc: 0.7153\n",
            "Epoch 6/99\n",
            "1077210/1077210 [==============================] - 165s 153us/step - loss: 0.7830 - acc: 0.7006 - val_loss: 0.7504 - val_acc: 0.7115\n",
            "Epoch 7/99\n",
            "1077210/1077210 [==============================] - 165s 153us/step - loss: 0.7767 - acc: 0.7023 - val_loss: 0.7534 - val_acc: 0.7131\n",
            "Epoch 8/99\n",
            "1077210/1077210 [==============================] - 164s 153us/step - loss: 0.7693 - acc: 0.7046 - val_loss: 0.7323 - val_acc: 0.7183\n",
            "Epoch 9/99\n",
            "1077210/1077210 [==============================] - 164s 153us/step - loss: 0.7639 - acc: 0.7068 - val_loss: 0.7276 - val_acc: 0.7211\n",
            "Epoch 10/99\n",
            "1077210/1077210 [==============================] - 165s 153us/step - loss: 0.7604 - acc: 0.7078 - val_loss: 0.7302 - val_acc: 0.7204\n",
            "Epoch 11/99\n",
            "1077210/1077210 [==============================] - 165s 153us/step - loss: 0.7554 - acc: 0.7090 - val_loss: 0.7301 - val_acc: 0.7198\n",
            "Epoch 12/99\n",
            "1077210/1077210 [==============================] - 165s 153us/step - loss: 0.7534 - acc: 0.7101 - val_loss: 0.7214 - val_acc: 0.7259\n",
            "Epoch 13/99\n",
            "1077210/1077210 [==============================] - 164s 152us/step - loss: 0.7506 - acc: 0.7114 - val_loss: 0.7297 - val_acc: 0.7180\n",
            "Epoch 14/99\n",
            "1077210/1077210 [==============================] - 164s 152us/step - loss: 0.7476 - acc: 0.7124 - val_loss: 0.7189 - val_acc: 0.7228\n",
            "Epoch 15/99\n",
            "1077210/1077210 [==============================] - 164s 153us/step - loss: 0.7459 - acc: 0.7129 - val_loss: 0.7273 - val_acc: 0.7206\n",
            "Epoch 16/99\n",
            "1077210/1077210 [==============================] - 164s 153us/step - loss: 0.7434 - acc: 0.7138 - val_loss: 0.7133 - val_acc: 0.7254\n",
            "Epoch 17/99\n",
            "1077210/1077210 [==============================] - 165s 153us/step - loss: 0.7423 - acc: 0.7145 - val_loss: 0.7434 - val_acc: 0.7188\n",
            "Epoch 18/99\n",
            "1077210/1077210 [==============================] - 164s 152us/step - loss: 0.7405 - acc: 0.7149 - val_loss: 0.7121 - val_acc: 0.7263\n",
            "Epoch 19/99\n",
            "1077210/1077210 [==============================] - 165s 153us/step - loss: 0.7391 - acc: 0.7153 - val_loss: 0.7068 - val_acc: 0.7292\n",
            "Epoch 20/99\n",
            "1077210/1077210 [==============================] - 164s 152us/step - loss: 0.7375 - acc: 0.7162 - val_loss: 0.7027 - val_acc: 0.7278\n",
            "Epoch 21/99\n",
            "1077210/1077210 [==============================] - 165s 153us/step - loss: 0.7358 - acc: 0.7170 - val_loss: 0.7088 - val_acc: 0.7295\n",
            "Epoch 22/99\n",
            "1077210/1077210 [==============================] - 164s 152us/step - loss: 0.7348 - acc: 0.7173 - val_loss: 0.7067 - val_acc: 0.7270\n",
            "Epoch 23/99\n",
            "1077210/1077210 [==============================] - 165s 153us/step - loss: 0.7344 - acc: 0.7174 - val_loss: 0.7095 - val_acc: 0.7266\n",
            "Epoch 24/99\n",
            "1077210/1077210 [==============================] - 163s 152us/step - loss: 0.7330 - acc: 0.7177 - val_loss: 0.7005 - val_acc: 0.7310\n",
            "Epoch 25/99\n",
            "1077210/1077210 [==============================] - 164s 152us/step - loss: 0.7322 - acc: 0.7185 - val_loss: 0.7360 - val_acc: 0.7193\n",
            "Epoch 26/99\n",
            "1077210/1077210 [==============================] - 163s 151us/step - loss: 0.7310 - acc: 0.7190 - val_loss: 0.7055 - val_acc: 0.7304\n",
            "Epoch 27/99\n",
            "1077210/1077210 [==============================] - 165s 153us/step - loss: 0.7316 - acc: 0.7187 - val_loss: 0.7024 - val_acc: 0.7313\n",
            "Epoch 28/99\n",
            "1077210/1077210 [==============================] - 163s 151us/step - loss: 0.7299 - acc: 0.7193 - val_loss: 0.7038 - val_acc: 0.7311\n",
            "Epoch 29/99\n",
            "1077210/1077210 [==============================] - 163s 152us/step - loss: 0.7294 - acc: 0.7194 - val_loss: 0.7052 - val_acc: 0.7284\n",
            "Accuracy in the val. set: 0.7309861717612809\n",
            "Accuracy in the test set: 0.7286208151382824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gbhkqEtnbwEV"
      },
      "source": [
        "### Model 1: Pre-trained 200D GloVe embedding layer + 1D convonlutional layer + 5 GRU layers + LSTM layer, with 1) fixed embedding weights and fine-tuning and 2) layer normalization for recurrent layers.¶\n",
        "\n",
        "#### According to our experiments, adding an additional LSTM layer makes the model performs better. The first intuition is that the hidden layers that come after are supposed to learn features that are at a higher hierachy. Thus, LSTM might be a better solution than GRU, for it entails an additional gate and thus entails more parameters. The first hidden layers, on the other hand, should learn the features from a lower hierachy, and GRUs (and the convolutional layer as well) might be more efficient than the LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ooc2b-5Q13DC",
        "outputId": "667fb536-662b-450d-9822-8fd69c997842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "inputs = Input(shape=(256,), dtype='int32')\n",
        "x = Embedding(max_words, embedding_dim, input_length=maxlen)(inputs)\n",
        "\n",
        "x = Conv1D(256, \n",
        "           7,\n",
        "           kernel_regularizer=l2(0.01),\n",
        "           kernel_initializer=keras.initializers.he_normal(seed=42))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling1D(5)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "x = GRU(64,\n",
        "        return_sequences=True, # to make sure the input in the next recurrent layer has the same dimensions as the original input           \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "x = GRU(64,\n",
        "        return_sequences=True,           \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "x = GRU(64,\n",
        "        return_sequences=True,           \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "x = GRU(32,\n",
        "        return_sequences=True,           \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "\n",
        "x = LSTM(32,\n",
        "         return_sequences=False,           \n",
        "         kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "         dropout=0.2, \n",
        "         recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "\n",
        "outputs = Dense(5, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# load the pre-trained embeddings into the first Embedding layer\n",
        "model.layers[1].set_weights([embedding_matrix])\n",
        "\n",
        "# set the Embedding layer to be non-trainable, preventing the pre-trained embeddings updating in the training process\n",
        "model.layers[1].trainable = False\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.0002),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train,\n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=2, \n",
        "          batch_size=2000)\n",
        "\n",
        "cb = EarlyStopping(monitor='val_loss', \n",
        "                   mode='min', \n",
        "                   verbose=0, \n",
        "                   patience=5,\n",
        "                   restore_best_weights=True)\n",
        "\n",
        "# fine-tuning the pre-trained Embedding layer\n",
        "model.layers[1].trainable = True\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.0001),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "history = model.fit(x=X_train, \n",
        "                    y=y_train,\n",
        "                    validation_data=[X_val, y_val],\n",
        "                    epochs=50, \n",
        "                    batch_size=2000,\n",
        "                    callbacks=[cb])\n",
        "\n",
        "score = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(\"Accuracy in the val. set:\", score[1])\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy in the test set:\", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1077210 samples, validate on 10992 samples\n",
            "Epoch 1/2\n",
            "1077210/1077210 [==============================] - 352s 327us/step - loss: 2.2203 - acc: 0.6245 - val_loss: 0.9234 - val_acc: 0.6850\n",
            "Epoch 2/2\n",
            "1077210/1077210 [==============================] - 346s 322us/step - loss: 0.8940 - acc: 0.6755 - val_loss: 0.7941 - val_acc: 0.7073\n",
            "Train on 1077210 samples, validate on 10992 samples\n",
            "Epoch 1/50\n",
            "1077210/1077210 [==============================] - 414s 384us/step - loss: 0.8145 - acc: 0.6938 - val_loss: 0.7471 - val_acc: 0.7168\n",
            "Epoch 2/50\n",
            "1077210/1077210 [==============================] - 407s 378us/step - loss: 0.7774 - acc: 0.7044 - val_loss: 0.7250 - val_acc: 0.7233\n",
            "Epoch 3/50\n",
            "1077210/1077210 [==============================] - 407s 378us/step - loss: 0.7542 - acc: 0.7116 - val_loss: 0.7071 - val_acc: 0.7316\n",
            "Epoch 4/50\n",
            "1077210/1077210 [==============================] - 408s 378us/step - loss: 0.7384 - acc: 0.7170 - val_loss: 0.7290 - val_acc: 0.7245\n",
            "Epoch 5/50\n",
            "1077210/1077210 [==============================] - 407s 378us/step - loss: 0.7265 - acc: 0.7208 - val_loss: 0.6971 - val_acc: 0.7310\n",
            "Epoch 6/50\n",
            "1077210/1077210 [==============================] - 407s 377us/step - loss: 0.7172 - acc: 0.7240 - val_loss: 0.6932 - val_acc: 0.7355\n",
            "Epoch 7/50\n",
            "1077210/1077210 [==============================] - 408s 378us/step - loss: 0.7100 - acc: 0.7266 - val_loss: 0.7000 - val_acc: 0.7342\n",
            "Epoch 8/50\n",
            "1077210/1077210 [==============================] - 406s 377us/step - loss: 0.7025 - acc: 0.7291 - val_loss: 0.7000 - val_acc: 0.7290\n",
            "Epoch 9/50\n",
            "1077210/1077210 [==============================] - 408s 379us/step - loss: 0.6983 - acc: 0.7304 - val_loss: 0.6774 - val_acc: 0.7431\n",
            "Epoch 10/50\n",
            "1077210/1077210 [==============================] - 411s 382us/step - loss: 0.6921 - acc: 0.7326 - val_loss: 0.6895 - val_acc: 0.7350\n",
            "Epoch 11/50\n",
            "1077210/1077210 [==============================] - 411s 381us/step - loss: 0.6886 - acc: 0.7337 - val_loss: 0.6737 - val_acc: 0.7446\n",
            "Epoch 12/50\n",
            "1077210/1077210 [==============================] - 412s 383us/step - loss: 0.6842 - acc: 0.7352 - val_loss: 0.6710 - val_acc: 0.7447\n",
            "Epoch 13/50\n",
            "1077210/1077210 [==============================] - 410s 381us/step - loss: 0.6817 - acc: 0.7363 - val_loss: 0.6829 - val_acc: 0.7396\n",
            "Epoch 14/50\n",
            "1077210/1077210 [==============================] - 409s 380us/step - loss: 0.6783 - acc: 0.7374 - val_loss: 0.6761 - val_acc: 0.7412\n",
            "Epoch 15/50\n",
            "1077210/1077210 [==============================] - 412s 382us/step - loss: 0.6753 - acc: 0.7386 - val_loss: 0.6688 - val_acc: 0.7455\n",
            "Epoch 16/50\n",
            "1077210/1077210 [==============================] - 413s 383us/step - loss: 0.6724 - acc: 0.7400 - val_loss: 0.6844 - val_acc: 0.7377\n",
            "Epoch 17/50\n",
            "1077210/1077210 [==============================] - 411s 382us/step - loss: 0.6705 - acc: 0.7406 - val_loss: 0.6672 - val_acc: 0.7433\n",
            "Epoch 18/50\n",
            "1077210/1077210 [==============================] - 411s 381us/step - loss: 0.6679 - acc: 0.7414 - val_loss: 0.6678 - val_acc: 0.7424\n",
            "Epoch 19/50\n",
            "1077210/1077210 [==============================] - 414s 384us/step - loss: 0.6657 - acc: 0.7421 - val_loss: 0.6681 - val_acc: 0.7431\n",
            "Epoch 20/50\n",
            "1077210/1077210 [==============================] - 413s 384us/step - loss: 0.6637 - acc: 0.7431 - val_loss: 0.6819 - val_acc: 0.7390\n",
            "Epoch 21/50\n",
            "1077210/1077210 [==============================] - 410s 381us/step - loss: 0.6609 - acc: 0.7443 - val_loss: 0.6705 - val_acc: 0.7435\n",
            "Accuracy in the val. set: 0.7432678311499272\n",
            "Accuracy in the test set: 0.7424490538573508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L6m1MlS2vrwC"
      },
      "source": [
        "### Model 2: Pre-trained 200D GloVe embedding layer + 1D convolutional layer + 5 GRU layers + LSTM layer, with 1) trainable, i.e. fine-tuned emmbedding weights and 2) layer normalization for recurrent layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4RaZm1dv2HZU",
        "outputId": "4a8e60dc-7a08-4a6c-8134-4bc068f55074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "inputs = Input(shape=(256,), dtype='int32')\n",
        "x = Embedding(max_words, embedding_dim, input_length=maxlen)(inputs)\n",
        "\n",
        "x = Conv1D(256, \n",
        "           7,\n",
        "           kernel_regularizer=l2(0.01),\n",
        "           kernel_initializer=keras.initializers.he_normal(seed=42))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling1D(5)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "x = GRU(64,\n",
        "        return_sequences=True,        \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "x = GRU(64,\n",
        "        return_sequences=True,           \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "x = GRU(64,\n",
        "        return_sequences=True,           \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "x = GRU(64,\n",
        "        return_sequences=True,           \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "x = GRU(32,\n",
        "        return_sequences=True,           \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "\n",
        "x = LSTM(32,\n",
        "         return_sequences=False,           \n",
        "         kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "         dropout=0.2, \n",
        "         recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "\n",
        "outputs = Dense(5, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.0001),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.layers[1].set_weights([embedding_matrix])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "cb = EarlyStopping(monitor='val_loss', \n",
        "                   mode='min', \n",
        "                   verbose=0, \n",
        "                   patience=5,\n",
        "                   restore_best_weights=True)\n",
        "\n",
        "history = model.fit(x=X_train, \n",
        "                    y=y_train,\n",
        "                    validation_data=[X_val, y_val],\n",
        "                    epochs=50, \n",
        "                    batch_size=2000,\n",
        "                    callbacks=[cb])\n",
        "\n",
        "score = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(\"Accuracy in the val. set:\", score[1])\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy in the test set:\", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 256, 200)          2000000   \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 250, 256)          358656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 250, 256)          1024      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 250, 256)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 50, 256)           0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 50, 256)           0         \n",
            "_________________________________________________________________\n",
            "gru_11 (GRU)                 (None, 50, 64)            61632     \n",
            "_________________________________________________________________\n",
            "layer_normalization_13 (Laye (None, 50, 64)            128       \n",
            "_________________________________________________________________\n",
            "gru_12 (GRU)                 (None, 50, 64)            24768     \n",
            "_________________________________________________________________\n",
            "layer_normalization_14 (Laye (None, 50, 64)            128       \n",
            "_________________________________________________________________\n",
            "gru_13 (GRU)                 (None, 50, 64)            24768     \n",
            "_________________________________________________________________\n",
            "layer_normalization_15 (Laye (None, 50, 64)            128       \n",
            "_________________________________________________________________\n",
            "gru_14 (GRU)                 (None, 50, 64)            24768     \n",
            "_________________________________________________________________\n",
            "layer_normalization_16 (Laye (None, 50, 64)            128       \n",
            "_________________________________________________________________\n",
            "gru_15 (GRU)                 (None, 50, 32)            9312      \n",
            "_________________________________________________________________\n",
            "layer_normalization_17 (Laye (None, 50, 32)            64        \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "layer_normalization_18 (Laye (None, 32)                64        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 2,514,053\n",
            "Trainable params: 2,513,541\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n",
            "Train on 1077210 samples, validate on 10992 samples\n",
            "Epoch 1/50\n",
            "1077210/1077210 [==============================] - 867s 805us/step - loss: 3.2891 - acc: 0.6072 - val_loss: 1.4890 - val_acc: 0.6372\n",
            "Epoch 2/50\n",
            "1077210/1077210 [==============================] - 865s 803us/step - loss: 1.1385 - acc: 0.6469 - val_loss: 0.8711 - val_acc: 0.6888\n",
            "Epoch 3/50\n",
            "1077210/1077210 [==============================] - 867s 805us/step - loss: 0.8670 - acc: 0.6831 - val_loss: 0.7742 - val_acc: 0.7122\n",
            "Epoch 4/50\n",
            "1077210/1077210 [==============================] - 869s 807us/step - loss: 0.8002 - acc: 0.6989 - val_loss: 0.7543 - val_acc: 0.7134\n",
            "Epoch 5/50\n",
            "1077210/1077210 [==============================] - 864s 802us/step - loss: 0.7697 - acc: 0.7066 - val_loss: 0.7232 - val_acc: 0.7248\n",
            "Epoch 6/50\n",
            "1077210/1077210 [==============================] - 864s 802us/step - loss: 0.7513 - acc: 0.7123 - val_loss: 0.7386 - val_acc: 0.7202\n",
            "Epoch 7/50\n",
            "1077210/1077210 [==============================] - 865s 803us/step - loss: 0.7375 - acc: 0.7165 - val_loss: 0.7000 - val_acc: 0.7297\n",
            "Epoch 8/50\n",
            "1077210/1077210 [==============================] - 861s 799us/step - loss: 0.7267 - acc: 0.7197 - val_loss: 0.6932 - val_acc: 0.7333\n",
            "Epoch 9/50\n",
            "1077210/1077210 [==============================] - 866s 804us/step - loss: 0.7181 - acc: 0.7231 - val_loss: 0.6906 - val_acc: 0.7369\n",
            "Epoch 10/50\n",
            "1077210/1077210 [==============================] - 866s 804us/step - loss: 0.7124 - acc: 0.7244 - val_loss: 0.6980 - val_acc: 0.7295\n",
            "Epoch 11/50\n",
            "1077210/1077210 [==============================] - 866s 804us/step - loss: 0.7064 - acc: 0.7270 - val_loss: 0.6935 - val_acc: 0.7318\n",
            "Epoch 12/50\n",
            "1077210/1077210 [==============================] - 872s 809us/step - loss: 0.7011 - acc: 0.7288 - val_loss: 0.6835 - val_acc: 0.7380\n",
            "Epoch 13/50\n",
            "1077210/1077210 [==============================] - 866s 804us/step - loss: 0.6974 - acc: 0.7302 - val_loss: 0.6772 - val_acc: 0.7381\n",
            "Epoch 14/50\n",
            "1077210/1077210 [==============================] - 867s 805us/step - loss: 0.6930 - acc: 0.7313 - val_loss: 0.6783 - val_acc: 0.7380\n",
            "Epoch 15/50\n",
            "1077210/1077210 [==============================] - 867s 805us/step - loss: 0.6900 - acc: 0.7328 - val_loss: 0.6790 - val_acc: 0.7419\n",
            "Epoch 16/50\n",
            "1077210/1077210 [==============================] - 870s 807us/step - loss: 0.6861 - acc: 0.7341 - val_loss: 0.6780 - val_acc: 0.7389\n",
            "Epoch 17/50\n",
            "1077210/1077210 [==============================] - 868s 806us/step - loss: 0.6834 - acc: 0.7348 - val_loss: 0.6758 - val_acc: 0.7395\n",
            "Epoch 18/50\n",
            "1077210/1077210 [==============================] - 868s 806us/step - loss: 0.6821 - acc: 0.7358 - val_loss: 0.7038 - val_acc: 0.7293\n",
            "Epoch 19/50\n",
            "1077210/1077210 [==============================] - 869s 807us/step - loss: 0.6783 - acc: 0.7371 - val_loss: 0.6787 - val_acc: 0.7399\n",
            "Epoch 20/50\n",
            "1077210/1077210 [==============================] - 873s 811us/step - loss: 0.6764 - acc: 0.7380 - val_loss: 0.6662 - val_acc: 0.7427\n",
            "Epoch 21/50\n",
            "1077210/1077210 [==============================] - 871s 808us/step - loss: 0.6748 - acc: 0.7380 - val_loss: 0.6883 - val_acc: 0.7353\n",
            "Epoch 22/50\n",
            "1077210/1077210 [==============================] - 869s 807us/step - loss: 0.6718 - acc: 0.7398 - val_loss: 0.6775 - val_acc: 0.7403\n",
            "Epoch 23/50\n",
            "1077210/1077210 [==============================] - 867s 805us/step - loss: 0.6702 - acc: 0.7400 - val_loss: 0.6847 - val_acc: 0.7362\n",
            "Epoch 24/50\n",
            "1077210/1077210 [==============================] - 870s 807us/step - loss: 0.6690 - acc: 0.7411 - val_loss: 0.6661 - val_acc: 0.7422\n",
            "Epoch 25/50\n",
            "1077210/1077210 [==============================] - 870s 808us/step - loss: 0.6664 - acc: 0.7420 - val_loss: 0.6764 - val_acc: 0.7405\n",
            "Epoch 26/50\n",
            "1077210/1077210 [==============================] - 870s 808us/step - loss: 0.6655 - acc: 0.7422 - val_loss: 0.6676 - val_acc: 0.7431\n",
            "Epoch 27/50\n",
            "1077210/1077210 [==============================] - 872s 809us/step - loss: 0.6633 - acc: 0.7430 - val_loss: 0.6879 - val_acc: 0.7366\n",
            "Epoch 28/50\n",
            "1077210/1077210 [==============================] - 872s 810us/step - loss: 0.6619 - acc: 0.7439 - val_loss: 0.6670 - val_acc: 0.7453\n",
            "Epoch 29/50\n",
            "1077210/1077210 [==============================] - 868s 806us/step - loss: 0.6604 - acc: 0.7442 - val_loss: 0.6678 - val_acc: 0.7447\n",
            "Accuracy in the val. set: 0.7421761280931587\n",
            "Accuracy in the test set: 0.7438136826783115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l3nFTDnmvw-K"
      },
      "source": [
        "### Model 3: RNN structure with residual connection: fixed 200D GloVe embedding + 1D convolutional layer + 5 GRU layers + LSTM layer, with a connection between the convolutional layer and the LSTM layer.\n",
        "\n",
        "#### In our 3. and 4. model, we tried to add an residual connection between the hidden layers. This is inspired by He et al. (2015), in whose design some hidden layers can be skipped in case feature representations at lower level have sufficient predictive power."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mpNW7ptH2SPA",
        "outputId": "d656ba5b-1e82-4c7f-d88c-c12af074f8e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "inputs = Input(shape=(256,), dtype='int32')\n",
        "x = Embedding(max_words, embedding_dim, input_length=maxlen)(inputs)\n",
        "\n",
        "x = Conv1D(256, \n",
        "           7,\n",
        "           kernel_regularizer=l2(0.01),\n",
        "           kernel_initializer=keras.initializers.he_normal(seed=42))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling1D(5)(x)\n",
        "cnv = Dropout(0.2)(x)\n",
        "\n",
        "x = GRU(64,\n",
        "        return_sequences=True,     \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(cnv)\n",
        "x = LayerNormalization()(x)\n",
        "x = GRU(64,\n",
        "        return_sequences=True,           \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "x = GRU(64,\n",
        "        return_sequences=True,           \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "x = GRU(64,\n",
        "        return_sequences=True,           \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "x = GRU(64,\n",
        "        return_sequences=True,           \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "\n",
        "x = concatenate([cnv, x])\n",
        "\n",
        "x = LSTM(64,\n",
        "         return_sequences=False,           \n",
        "         kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "         dropout=0.2, \n",
        "         recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "\n",
        "outputs = Dense(5, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.0002),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.layers[1].set_weights([embedding_matrix])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.layers[1].trainable = False\n",
        "\n",
        "model.fit(x=X_train, \n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=5,\n",
        "          batch_size=2000)\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.0001),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "\n",
        "cb = EarlyStopping(monitor='val_loss', \n",
        "                   mode='min', \n",
        "                   verbose=0, \n",
        "                   patience=5,\n",
        "                   restore_best_weights=True)\n",
        "\n",
        "history = model.fit(x=X_train, \n",
        "                    y=y_train,\n",
        "                    validation_data=[X_val, y_val],\n",
        "                    epochs=50, \n",
        "                    batch_size=2000,\n",
        "                    callbacks=[cb])\n",
        "\n",
        "score = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(\"Accuracy in the val. set:\", score[1])\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy in the test set:\", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0630 07:05:17.601340 140393608140672 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 256)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 256, 200)     3200000     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 250, 256)     358656      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 250, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 250, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 50, 256)      0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 50, 256)      0           max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "gru_6 (GRU)                     (None, 50, 64)       61632       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_6 (LayerNor (None, 50, 64)       128         gru_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru_7 (GRU)                     (None, 50, 64)       24768       layer_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_7 (LayerNor (None, 50, 64)       128         gru_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru_8 (GRU)                     (None, 50, 64)       24768       layer_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_8 (LayerNor (None, 50, 64)       128         gru_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru_9 (GRU)                     (None, 50, 64)       24768       layer_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_9 (LayerNor (None, 50, 64)       128         gru_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru_10 (GRU)                    (None, 50, 64)       24768       layer_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_10 (LayerNo (None, 50, 64)       128         gru_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 50, 320)      0           dropout_2[0][0]                  \n",
            "                                                                 layer_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 64)           98560       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_11 (LayerNo (None, 64)           128         lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 5)            325         layer_normalization_11[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 3,820,037\n",
            "Trainable params: 3,819,525\n",
            "Non-trainable params: 512\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "W0630 07:05:21.791081 140393608140672 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1077210 samples, validate on 10992 samples\n",
            "Epoch 1/5\n",
            "1077210/1077210 [==============================] - 472s 438us/step - loss: 2.3756 - acc: 0.6412 - val_loss: 0.9312 - val_acc: 0.6950\n",
            "Epoch 2/5\n",
            "1077210/1077210 [==============================] - 460s 427us/step - loss: 0.8422 - acc: 0.6987 - val_loss: 0.7605 - val_acc: 0.7222\n",
            "Epoch 3/5\n",
            "1077210/1077210 [==============================] - 458s 425us/step - loss: 0.7634 - acc: 0.7130 - val_loss: 0.7267 - val_acc: 0.7274\n",
            "Epoch 4/5\n",
            "1077210/1077210 [==============================] - 458s 425us/step - loss: 0.7355 - acc: 0.7204 - val_loss: 0.7113 - val_acc: 0.7293\n",
            "Epoch 5/5\n",
            "1077210/1077210 [==============================] - 458s 425us/step - loss: 0.7174 - acc: 0.7255 - val_loss: 0.7023 - val_acc: 0.7331\n",
            "Train on 1077210 samples, validate on 10992 samples\n",
            "Epoch 1/50\n",
            "1077210/1077210 [==============================] - 411s 381us/step - loss: 0.6950 - acc: 0.7318 - val_loss: 0.6880 - val_acc: 0.7355\n",
            "Epoch 2/50\n",
            "1077210/1077210 [==============================] - 402s 373us/step - loss: 0.6862 - acc: 0.7339 - val_loss: 0.6805 - val_acc: 0.7384\n",
            "Epoch 3/50\n",
            "1077210/1077210 [==============================] - 403s 374us/step - loss: 0.6825 - acc: 0.7357 - val_loss: 0.6836 - val_acc: 0.7384\n",
            "Epoch 4/50\n",
            "1077210/1077210 [==============================] - 404s 375us/step - loss: 0.6785 - acc: 0.7365 - val_loss: 0.6716 - val_acc: 0.7424\n",
            "Epoch 5/50\n",
            "1077210/1077210 [==============================] - 405s 376us/step - loss: 0.6765 - acc: 0.7373 - val_loss: 0.6881 - val_acc: 0.7324\n",
            "Epoch 6/50\n",
            "1077210/1077210 [==============================] - 404s 375us/step - loss: 0.6747 - acc: 0.7378 - val_loss: 0.6856 - val_acc: 0.7358\n",
            "Epoch 7/50\n",
            "1077210/1077210 [==============================] - 404s 375us/step - loss: 0.6731 - acc: 0.7382 - val_loss: 0.6677 - val_acc: 0.7432\n",
            "Epoch 8/50\n",
            "1077210/1077210 [==============================] - 405s 376us/step - loss: 0.6701 - acc: 0.7393 - val_loss: 0.6671 - val_acc: 0.7421\n",
            "Epoch 9/50\n",
            "1077210/1077210 [==============================] - 404s 375us/step - loss: 0.6685 - acc: 0.7402 - val_loss: 0.6669 - val_acc: 0.7416\n",
            "Epoch 10/50\n",
            "1077210/1077210 [==============================] - 403s 374us/step - loss: 0.6676 - acc: 0.7402 - val_loss: 0.6633 - val_acc: 0.7425\n",
            "Epoch 11/50\n",
            "1077210/1077210 [==============================] - 405s 376us/step - loss: 0.6662 - acc: 0.7410 - val_loss: 0.6834 - val_acc: 0.7373\n",
            "Epoch 12/50\n",
            "1077210/1077210 [==============================] - 403s 374us/step - loss: 0.6646 - acc: 0.7417 - val_loss: 0.6713 - val_acc: 0.7414\n",
            "Epoch 13/50\n",
            "1077210/1077210 [==============================] - 403s 374us/step - loss: 0.6631 - acc: 0.7424 - val_loss: 0.6820 - val_acc: 0.7365\n",
            "Epoch 14/50\n",
            "1077210/1077210 [==============================] - 404s 375us/step - loss: 0.6624 - acc: 0.7422 - val_loss: 0.6757 - val_acc: 0.7404\n",
            "Epoch 15/50\n",
            "1077210/1077210 [==============================] - 403s 374us/step - loss: 0.6608 - acc: 0.7435 - val_loss: 0.6603 - val_acc: 0.7451\n",
            "Epoch 16/50\n",
            "1077210/1077210 [==============================] - 403s 374us/step - loss: 0.6607 - acc: 0.7437 - val_loss: 0.6697 - val_acc: 0.7404\n",
            "Epoch 17/50\n",
            "1077210/1077210 [==============================] - 404s 375us/step - loss: 0.6597 - acc: 0.7438 - val_loss: 0.6623 - val_acc: 0.7463\n",
            "Epoch 18/50\n",
            "1077210/1077210 [==============================] - 404s 375us/step - loss: 0.6576 - acc: 0.7445 - val_loss: 0.6646 - val_acc: 0.7439\n",
            "Epoch 19/50\n",
            "1077210/1077210 [==============================] - 403s 374us/step - loss: 0.6568 - acc: 0.7451 - val_loss: 0.6621 - val_acc: 0.7445\n",
            "Epoch 20/50\n",
            "1077210/1077210 [==============================] - 403s 374us/step - loss: 0.6561 - acc: 0.7452 - val_loss: 0.6675 - val_acc: 0.7458\n",
            "Accuracy in the val. set: 0.7450873362445415\n",
            "Accuracy in the test set: 0.7445414847161572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SDS_WVpRvyxj"
      },
      "source": [
        "### Model 4: RNN structure with residual connection: pre-trained 200D GloVe embedding layer + 1D convonlutional layer + 5 GRU layers + LSTM layer, with 1) fixed embedding weights and fine-tuning,  2) layer normalization for recurrent layers and 3) a connection between the convolutional layer and the last GRU layer, followed by an additional LSTM layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1e8jjIEiWLFj",
        "outputId": "9c177156-0bb9-484b-da50-6816cba1abf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "inputs = Input(shape=(256,), dtype='int32')\n",
        "x = Embedding(max_words, embedding_dim, input_length=maxlen)(inputs)\n",
        "\n",
        "x = Conv1D(256, \n",
        "           7,\n",
        "           kernel_regularizer=l2(0.01),\n",
        "           kernel_initializer=keras.initializers.he_normal(seed=42))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = MaxPooling1D(5)(x)\n",
        "cnv = Dropout(0.2)(x)\n",
        "\n",
        "x = GRU(64,\n",
        "        return_sequences=True,        \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(cnv)\n",
        "x = LayerNormalization()(x)\n",
        "x = GRU(64,\n",
        "        return_sequences=True,           \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "x = GRU(64,\n",
        "        return_sequences=True,           \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "\n",
        "x = concatenate([cnv, x])\n",
        "\n",
        "x = GRU(32,\n",
        "        return_sequences=True,           \n",
        "        kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "        dropout=0.2, \n",
        "        recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "\n",
        "x = LSTM(32,\n",
        "         return_sequences=False,           \n",
        "         kernel_initializer=keras.initializers.Orthogonal(seed=42), \n",
        "         dropout=0.2, \n",
        "         recurrent_dropout=0.2)(x)\n",
        "x = LayerNormalization()(x)\n",
        "\n",
        "outputs = Dense(5, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.layers[1].set_weights([embedding_matrix])\n",
        "\n",
        "model.layers[1].trainable = False\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.0002),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x=X_train,\n",
        "          y=y_train,\n",
        "          validation_data=[X_val, y_val],\n",
        "          epochs=5, \n",
        "          batch_size=2000)\n",
        "\n",
        "cb = EarlyStopping(monitor='val_loss', \n",
        "                   mode='min', \n",
        "                   verbose=0, \n",
        "                   patience=5,\n",
        "                   restore_best_weights=True)\n",
        "\n",
        "model.layers[1].trainable = True\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.0001),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "history = model.fit(x=X_train, \n",
        "                    y=y_train,\n",
        "                    validation_data=[X_val, y_val],\n",
        "                    epochs=50, \n",
        "                    batch_size=2000,\n",
        "                    callbacks=[cb])\n",
        "\n",
        "score = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(\"Accuracy in the val. set:\", score[1])\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy in the test set:\", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 256)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 256, 200)     3200000     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 250, 256)     358656      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 250, 256)     1024        conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 250, 256)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 50, 256)      0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 50, 256)      0           max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "gru_5 (GRU)                     (None, 50, 64)       61632       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_6 (LayerNor (None, 50, 64)       128         gru_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru_6 (GRU)                     (None, 50, 64)       24768       layer_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_7 (LayerNor (None, 50, 64)       128         gru_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru_7 (GRU)                     (None, 50, 64)       24768       layer_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_8 (LayerNor (None, 50, 64)       128         gru_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 50, 320)      0           dropout_2[0][0]                  \n",
            "                                                                 layer_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "gru_8 (GRU)                     (None, 50, 32)       33888       concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_9 (LayerNor (None, 50, 32)       64          gru_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 32)           8320        layer_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_10 (LayerNo (None, 32)           64          lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 5)            165         layer_normalization_10[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 3,713,733\n",
            "Trainable params: 3,713,221\n",
            "Non-trainable params: 512\n",
            "__________________________________________________________________________________________________\n",
            "Train on 1077210 samples, validate on 10992 samples\n",
            "Epoch 1/5\n",
            "1077210/1077210 [==============================] - 367s 341us/step - loss: 2.3041 - acc: 0.6327 - val_loss: 0.9242 - val_acc: 0.6920\n",
            "Epoch 2/5\n",
            "1077210/1077210 [==============================] - 358s 333us/step - loss: 0.8876 - acc: 0.6828 - val_loss: 0.7961 - val_acc: 0.7042\n",
            "Epoch 3/5\n",
            "1077210/1077210 [==============================] - 357s 332us/step - loss: 0.8248 - acc: 0.6934 - val_loss: 0.8033 - val_acc: 0.7012\n",
            "Epoch 4/5\n",
            "1077210/1077210 [==============================] - 357s 331us/step - loss: 0.8029 - acc: 0.6984 - val_loss: 0.7718 - val_acc: 0.7051\n",
            "Epoch 5/5\n",
            "1077210/1077210 [==============================] - 357s 331us/step - loss: 0.7870 - acc: 0.7031 - val_loss: 0.7658 - val_acc: 0.7053\n",
            "Train on 1077210 samples, validate on 10992 samples\n",
            "Epoch 1/50\n",
            "1077210/1077210 [==============================] - 430s 399us/step - loss: 0.7579 - acc: 0.7117 - val_loss: 0.7158 - val_acc: 0.7284\n",
            "Epoch 2/50\n",
            "1077210/1077210 [==============================] - 421s 391us/step - loss: 0.7334 - acc: 0.7189 - val_loss: 0.7091 - val_acc: 0.7293\n",
            "Epoch 3/50\n",
            "1077210/1077210 [==============================] - 420s 390us/step - loss: 0.7200 - acc: 0.7233 - val_loss: 0.6896 - val_acc: 0.7379\n",
            "Epoch 4/50\n",
            "1077210/1077210 [==============================] - 419s 389us/step - loss: 0.7087 - acc: 0.7272 - val_loss: 0.7131 - val_acc: 0.7282\n",
            "Epoch 5/50\n",
            "1077210/1077210 [==============================] - 418s 388us/step - loss: 0.6998 - acc: 0.7303 - val_loss: 0.6864 - val_acc: 0.7396\n",
            "Epoch 6/50\n",
            "1077210/1077210 [==============================] - 418s 388us/step - loss: 0.6934 - acc: 0.7323 - val_loss: 0.6881 - val_acc: 0.7341\n",
            "Epoch 7/50\n",
            "1077210/1077210 [==============================] - 419s 389us/step - loss: 0.6868 - acc: 0.7345 - val_loss: 0.6715 - val_acc: 0.7444\n",
            "Epoch 8/50\n",
            "1077210/1077210 [==============================] - 419s 389us/step - loss: 0.6821 - acc: 0.7362 - val_loss: 0.6805 - val_acc: 0.7400\n",
            "Epoch 9/50\n",
            "1077210/1077210 [==============================] - 421s 391us/step - loss: 0.6775 - acc: 0.7385 - val_loss: 0.6728 - val_acc: 0.7451\n",
            "Epoch 10/50\n",
            "1077210/1077210 [==============================] - 420s 390us/step - loss: 0.6730 - acc: 0.7399 - val_loss: 0.6749 - val_acc: 0.7404\n",
            "Epoch 11/50\n",
            "1077210/1077210 [==============================] - 416s 386us/step - loss: 0.6695 - acc: 0.7409 - val_loss: 0.6837 - val_acc: 0.7359\n",
            "Epoch 12/50\n",
            "1077210/1077210 [==============================] - 416s 386us/step - loss: 0.6661 - acc: 0.7422 - val_loss: 0.6668 - val_acc: 0.7484\n",
            "Epoch 13/50\n",
            "1077210/1077210 [==============================] - 417s 387us/step - loss: 0.6635 - acc: 0.7433 - val_loss: 0.6701 - val_acc: 0.7475\n",
            "Epoch 14/50\n",
            "1077210/1077210 [==============================] - 415s 386us/step - loss: 0.6610 - acc: 0.7445 - val_loss: 0.6747 - val_acc: 0.7411\n",
            "Epoch 15/50\n",
            "1077210/1077210 [==============================] - 413s 384us/step - loss: 0.6580 - acc: 0.7451 - val_loss: 0.6645 - val_acc: 0.7455\n",
            "Epoch 16/50\n",
            "1077210/1077210 [==============================] - 414s 384us/step - loss: 0.6557 - acc: 0.7462 - val_loss: 0.6666 - val_acc: 0.7432\n",
            "Epoch 17/50\n",
            "1077210/1077210 [==============================] - 413s 384us/step - loss: 0.6533 - acc: 0.7472 - val_loss: 0.6726 - val_acc: 0.7447\n",
            "Epoch 18/50\n",
            "1077210/1077210 [==============================] - 418s 388us/step - loss: 0.6518 - acc: 0.7477 - val_loss: 0.6642 - val_acc: 0.7437\n",
            "Epoch 19/50\n",
            "1077210/1077210 [==============================] - 416s 386us/step - loss: 0.6496 - acc: 0.7487 - val_loss: 0.6617 - val_acc: 0.7481\n",
            "Epoch 20/50\n",
            "1077210/1077210 [==============================] - 415s 385us/step - loss: 0.6471 - acc: 0.7497 - val_loss: 0.6598 - val_acc: 0.7469\n",
            "Epoch 21/50\n",
            "1077210/1077210 [==============================] - 417s 387us/step - loss: 0.6459 - acc: 0.7501 - val_loss: 0.6607 - val_acc: 0.7475\n",
            "Epoch 22/50\n",
            "1077210/1077210 [==============================] - 419s 389us/step - loss: 0.6438 - acc: 0.7509 - val_loss: 0.6642 - val_acc: 0.7465\n",
            "Epoch 23/50\n",
            "1077210/1077210 [==============================] - 419s 389us/step - loss: 0.6420 - acc: 0.7519 - val_loss: 0.6652 - val_acc: 0.7461\n",
            "Epoch 24/50\n",
            "1077210/1077210 [==============================] - 419s 389us/step - loss: 0.6411 - acc: 0.7518 - val_loss: 0.6635 - val_acc: 0.7460\n",
            "Epoch 25/50\n",
            "1077210/1077210 [==============================] - 419s 389us/step - loss: 0.6391 - acc: 0.7532 - val_loss: 0.6703 - val_acc: 0.7447\n",
            "Accuracy in the val. set: 0.7469068413391557\n",
            "Accuracy in the test set: 0.7469068413391557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rgWAIdNh5QU",
        "colab_type": "text"
      },
      "source": [
        "#### All models are saved in the Drive and will be loaded and evaluated in our next notebook."
      ]
    }
  ]
}